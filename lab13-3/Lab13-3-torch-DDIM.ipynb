{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TODO: change ID\n",
    "ID = \"ddim-v1\"\n",
    "\n",
    "MIXED_PRECISION = True\n",
    "# TODO: choose distributed or not\n",
    "DISTRIBUTED = True\n",
    "RANK = int(os.getenv(\"RANK\", 0))\n",
    "WORLD_SIZE = int(os.getenv(\"WORLD_SIZE\", 1))\n",
    "\n",
    "# TODO: change device\n",
    "DEVICE_ID = 0  # None for CPU\n",
    "DEVICE_IDS = [0, 1, 2, 3]\n",
    "OMP_NUM_THREADS = 10\n",
    "SEED = 42\n",
    "\n",
    "# Dataset\n",
    "DATASET_REPETITIONS = 5\n",
    "IMAGE_SIZE = 64\n",
    "\n",
    "# KID = Kernel Inception Distance, see related section\n",
    "KID_IMAGE_SIZE = 75\n",
    "KID_DIFFUSION_STEPS = 5\n",
    "PLOT_DIFFUSION_STEPS = 20\n",
    "\n",
    "# sampling\n",
    "MIN_SIGNAL_RATE = 0.02\n",
    "MAX_SIGNAL_RATE = 0.95\n",
    "\n",
    "# architecture\n",
    "EMBEDDING_DIM = 32\n",
    "EMBEDDING_MAX_FREQUENCY = 1000.0\n",
    "WIDTHS = [32, 64, 96, 128]\n",
    "BLOCK_DEPTH = 2\n",
    "\n",
    "# training\n",
    "# TODO: change epochs\n",
    "START_EPOCH = 0\n",
    "EPOCHS = 10\n",
    "PLOT_EVERY = 1\n",
    "BATCH_SIZE = 256 // WORLD_SIZE\n",
    "LEARNING_RATE = 1e-3\n",
    "EMA = 0.998  # TODO: change EMA\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "CHECKPOINT_DIR = os.path.join(\"./ckpts/\", ID)\n",
    "CHECKPOINT_NAME = \"ckpt\"\n",
    "OUTPUT_DIR = os.path.join(\"./outputs/\", ID)\n",
    "DISTRIBUTED_STORE = os.path.join(\"./dist_store/\", ID)\n",
    "SAVE_PLOTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    gpus = torch.cuda.device_count()\n",
    "    DEVICE_ID = DEVICE_ID if DEVICE_ID < gpus else 0\n",
    "    if DISTRIBUTED:\n",
    "        DEVICE_IDS = [id for id in DEVICE_IDS if id < gpus]\n",
    "    else:\n",
    "        for device_id in DEVICE_IDS:\n",
    "            if device_id >= gpus:\n",
    "                raise ValueError(f\"GPU {device_id} is not available.\")\n",
    "else:\n",
    "    DEVICE_ID = None\n",
    "\n",
    "if DISTRIBUTED:\n",
    "    print(f\"Rank {RANK} Using distributed training with devices: {DEVICE_IDS}\")\n",
    "else:\n",
    "    print(f\"Using device id: {DEVICE_ID}\")\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(OMP_NUM_THREADS)\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Load the Flowers102 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython import get_ipython\n",
    "\n",
    "\n",
    "def transform_clamp(x):\n",
    "    return torch.clamp(x, 0, 1)\n",
    "\n",
    "\n",
    "def get_dataloader(\n",
    "    batch_size: int,\n",
    "    image_size: int,\n",
    "    split: str = \"train\",\n",
    "    repeats: int = 1,\n",
    "    pin_memory: bool = True,\n",
    "    num_workers: int = 4,\n",
    "    shuffle: bool = False,\n",
    "    rank: int = 0,\n",
    "    distributed: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the Flowers102 dataset.\n",
    "\n",
    "    :param batch_size: The batch size.\n",
    "    :param split: The split to load. Either \"train\", \"val\", or \"test\".\n",
    "    :param num_workers: The number of workers to use for loading the data.\n",
    "    :param shuffle: Whether to shuffle the data.\n",
    "    \"\"\"\n",
    "    if split not in [\"train\", \"val\", \"test\"]:\n",
    "        raise ValueError(f\"Invalid split: {split}\")\n",
    "    if split == \"train\":\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                # transforms.RandomResizedCrop(image_size, scale=(0.9, 1.0)),\n",
    "                transforms.Resize(image_size),\n",
    "                transforms.CenterCrop(image_size),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transform_clamp,\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(int(image_size * 1.1)),\n",
    "                transforms.CenterCrop(image_size),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "    dataset = datasets.Flowers102(\n",
    "        root=\"datasets/flowers102\",\n",
    "        split=split,\n",
    "        transform=transform,\n",
    "        download=True,\n",
    "    )\n",
    "    if split == \"train\":\n",
    "        test_dataset = datasets.Flowers102(\n",
    "            root=\"datasets/flowers102\",\n",
    "            split=\"test\",\n",
    "            transform=transform,\n",
    "            download=True,\n",
    "        )\n",
    "        dataset = torch.utils.data.ConcatDataset([dataset] + [test_dataset])\n",
    "    dataset = torch.utils.data.ConcatDataset([dataset] * repeats)\n",
    "    if distributed:\n",
    "        sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "            dataset,\n",
    "            rank=rank,\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "        shuffle = None  # shuffle is mutually exclusive with sampler\n",
    "    else:\n",
    "        sampler = None\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        num_workers=num_workers,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "ipy = get_ipython()\n",
    "if ipy is not None:\n",
    "    # If running in a notebook, load the data and show some images\n",
    "    ipy.run_line_magic(\"matplotlib\", \"inline\")\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    train_loader = get_dataloader(32, IMAGE_SIZE, split=\"train\", shuffle=True)\n",
    "\n",
    "    # Get a batch of data\n",
    "    inputs, classes = next(iter(train_loader))\n",
    "\n",
    "    # Make a grid from batch\n",
    "    out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "    def imshow(inp, title=None):\n",
    "        \"\"\"Imshow for Tensor.\"\"\"\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        inp = inp.numpy().transpose((1, 2, 0))\n",
    "        inp = np.clip(inp, 0, 1)\n",
    "        plt.imshow(inp)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "\n",
    "    imshow(out)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel inception distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torchmetrics import Metric\n",
    "\n",
    "\n",
    "class KID(Metric):\n",
    "    def __init__(self, kid_image_size=75, **kwargs):\n",
    "        super(KID, self).__init__(**kwargs)\n",
    "\n",
    "        # self.transforms = models.Inception_V3_Weights.DEFAULT.transforms()\n",
    "        self.transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(kid_image_size),\n",
    "                transforms.CenterCrop(kid_image_size),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        encoder: nn.Module = models.inception_v3(\n",
    "            weights=models.Inception_V3_Weights.DEFAULT\n",
    "        )\n",
    "        encoder.dropout = (\n",
    "            nn.Identity()\n",
    "        )  # Replace the dropout with an identity function\n",
    "        encoder.fc = (\n",
    "            nn.Identity()\n",
    "        )  # Replace the classifier with an identity function\n",
    "        for param in encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.encoder = encoder\n",
    "        self.encoder.eval()\n",
    "\n",
    "        # Mean of kid\n",
    "        self.add_state(\"kid\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\n",
    "            \"count\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\"\n",
    "        )\n",
    "\n",
    "    def polynomial_kernel(\n",
    "        self, x1: torch.Tensor, x2: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the polynomial kernel between two sets of features.\n",
    "\n",
    "        :param x1 torch.Tensor: The first set of features. Shape (batch_size, feat_dim).\n",
    "        :param x2 torch.Tensor: The second set of features. Shape (batch_size, feat_dim).\n",
    "        :return: The kernel matrix. Shape (batch_size, batch_size).\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        feat_dim = x1.shape[1]\n",
    "        output = (x1 @ x2.T / feat_dim + 1.0) ** 3.0\n",
    "        return output\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor) -> None:\n",
    "        assert preds.shape == target.shape\n",
    "        assert preds.device == target.device\n",
    "        assert (\n",
    "            preds.device == self.device\n",
    "        ), f\"Expected device {self.device}, got {preds.device}\"\n",
    "        batch_size = preds.shape[0]\n",
    "\n",
    "        # Transform the images using the InceptionV3 preprocessing\n",
    "        target = self.transforms(target)\n",
    "        preds = self.transforms(preds)\n",
    "\n",
    "        # Compute the features using the InceptionV3 encoder\n",
    "        real_features: torch.Tensor = self.encoder(\n",
    "            target\n",
    "        )  # (batch_size, feat_dim)\n",
    "        generated_features: torch.Tensor = self.encoder(\n",
    "            preds\n",
    "        )  # (batch_size, feat_dim)\n",
    "\n",
    "        # compute polynomial kernels using the two sets of features\n",
    "        real_kernel = self.polynomial_kernel(real_features, real_features)\n",
    "        generated_kernel = self.polynomial_kernel(\n",
    "            generated_features, generated_features\n",
    "        )\n",
    "        cross_kernel = self.polynomial_kernel(\n",
    "            real_features, generated_features\n",
    "        )\n",
    "\n",
    "        # estimate the squared maximum mean discrepancy using the average kernel values\n",
    "        eye = torch.eye(batch_size).cuda()\n",
    "        real_mean = (real_kernel * (1.0 - eye)).sum() / (\n",
    "            batch_size * (batch_size - 1)\n",
    "        )\n",
    "        generated_mean = (generated_kernel * (1.0 - eye)).sum() / (\n",
    "            batch_size * (batch_size - 1)\n",
    "        )\n",
    "        cross_mean = cross_kernel.sum() / batch_size**2\n",
    "\n",
    "        # Calculate KID\n",
    "        kid = real_mean + generated_mean - 2.0 * cross_mean\n",
    "        self.kid += kid\n",
    "        self.count += 1\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        return self.kid / self.count\n",
    "\n",
    "\n",
    "def test_kid():\n",
    "    torch.cuda.set_device(DEVICE_ID)\n",
    "    kid = KID(kid_image_size=KID_IMAGE_SIZE).cuda()\n",
    "\n",
    "    train_loader = get_dataloader(128, IMAGE_SIZE, split=\"train\")\n",
    "    images, _ = next(iter(train_loader))\n",
    "    images = images.cuda()\n",
    "\n",
    "    kid.update(images, images)\n",
    "    val = kid.compute()\n",
    "    print(\"kid value:\", val.item())\n",
    "    kid.reset()\n",
    "\n",
    "    sample_preds = torch.randn(128, 3, IMAGE_SIZE, IMAGE_SIZE).cuda()\n",
    "    sample_target = torch.randn(128, 3, IMAGE_SIZE, IMAGE_SIZE).cuda()\n",
    "\n",
    "    kid.update(images, sample_target)\n",
    "    val = kid.compute()\n",
    "    print(\"kid value:\", val.item())\n",
    "    kid.reset()\n",
    "\n",
    "    kid.update(sample_preds, sample_target)\n",
    "    val = kid.compute()\n",
    "    print(\"kid value:\", val.item())\n",
    "    kid.reset()\n",
    "\n",
    "\n",
    "# test_kid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_min_frequency=1.0,\n",
    "        embedding_max_frequency=1000.0,\n",
    "        embedding_dims=32,\n",
    "    ):\n",
    "        super(SinusoidalEmbedding, self).__init__()\n",
    "        assert embedding_dims % 2 == 0, \"Embedding dimensions must be even\"\n",
    "        self.embedding_min_frequency = embedding_min_frequency\n",
    "        self.embedding_max_frequency = embedding_max_frequency\n",
    "        self.embedding_dims = embedding_dims\n",
    "\n",
    "        frequencies = torch.exp(\n",
    "            torch.linspace(\n",
    "                torch.log(torch.tensor(self.embedding_min_frequency)),\n",
    "                torch.log(torch.tensor(self.embedding_max_frequency)),\n",
    "                self.embedding_dims // 2,\n",
    "            )\n",
    "        )\n",
    "        angular_speeds = 2.0 * torch.pi * frequencies\n",
    "        angular_speeds = angular_speeds.view(1, -1, 1, 1)\n",
    "        self.angular_speeds = nn.Parameter(angular_speeds, requires_grad=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        embeddings = torch.cat(\n",
    "            [\n",
    "                torch.sin(self.angular_speeds * x),\n",
    "                torch.cos(self.angular_speeds * x),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        if in_channels != out_channels:\n",
    "            self.residual_conv = nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size=1\n",
    "            )\n",
    "        else:\n",
    "            self.residual_conv = nn.Identity()\n",
    "        self.norm1 = nn.BatchNorm2d(in_channels, eps=1e-8, momentum=0.01)\n",
    "        self.relu = nn.SiLU()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            nn.init.xavier_normal_(\n",
    "                module.weight, gain=nn.init.calculate_gain(\"selu\")\n",
    "            )\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        residual = self.residual_conv(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x + residual\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, block_depth):\n",
    "        super(DownBlock, self).__init__()\n",
    "        assert block_depth > 0, \"Block depth must be greater than 0\"\n",
    "        self.block_depth = block_depth\n",
    "        self.residual_blocks = nn.ModuleList(\n",
    "            [ResidualBlock(in_channels, out_channels)]\n",
    "            + [\n",
    "                ResidualBlock(out_channels, out_channels)\n",
    "                for _ in range(block_depth - 1)\n",
    "            ]\n",
    "        )\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, skips: list):\n",
    "        for block in self.residual_blocks:\n",
    "            x = block(x)\n",
    "            skips.append(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, block_depth):\n",
    "        super(UpBlock, self).__init__()\n",
    "        assert block_depth > 0, \"Block depth must be greater than 0\"\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.block_depth = block_depth\n",
    "\n",
    "        self.up = nn.Upsample(\n",
    "            scale_factor=2, mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "        self.residual_blocks = nn.ModuleList(\n",
    "            [ResidualBlock(in_channels + out_channels, out_channels)]\n",
    "            + [\n",
    "                ResidualBlock(out_channels * 2, out_channels)\n",
    "                for _ in range(block_depth - 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, skips: list):\n",
    "        assert x.shape[1] == self.in_channels\n",
    "        x = self.up(x)\n",
    "        for block in self.residual_blocks:\n",
    "            x = torch.cat([x, skips.pop()], dim=1)\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualUNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        widths,\n",
    "        block_depth,\n",
    "        embedding_dims,\n",
    "        embedding_max_frequency,\n",
    "    ):\n",
    "        super(ResidualUNet, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.widths = widths\n",
    "        self.embedding_dims = embedding_dims\n",
    "\n",
    "        # Initialize the components\n",
    "        self.sinusoidal_embedding = SinusoidalEmbedding(\n",
    "            embedding_max_frequency=embedding_max_frequency,\n",
    "            embedding_dims=embedding_dims,\n",
    "        )\n",
    "\n",
    "        # Initial Conv2d layer\n",
    "        self.conv1 = nn.Conv2d(3, widths[0], kernel_size=1)\n",
    "\n",
    "        # Down blocks\n",
    "        self.down_blocks = nn.ModuleList(\n",
    "            [\n",
    "                DownBlock(\n",
    "                    self.image_size // 2 + self.embedding_dims,\n",
    "                    widths[0],\n",
    "                    block_depth=block_depth,\n",
    "                )\n",
    "            ]\n",
    "            + [\n",
    "                DownBlock(widths[i - 1], widths[i], block_depth=block_depth)\n",
    "                for i in range(1, len(widths) - 1)\n",
    "            ]\n",
    "        )\n",
    "        # Residual blocks in the bottleneck\n",
    "        self.bottleneck = nn.ModuleList(\n",
    "            [ResidualBlock(widths[-2], widths[-1])]\n",
    "            + [\n",
    "                ResidualBlock(widths[-1], widths[-1])\n",
    "                for _ in range(block_depth - 1)\n",
    "            ]\n",
    "        )\n",
    "        # Up blocks\n",
    "        self.up_blocks = nn.ModuleList(\n",
    "            [UpBlock(widths[-1], widths[-2], block_depth=block_depth)]\n",
    "            + [\n",
    "                UpBlock(widths[i], widths[i - 1], block_depth=block_depth)\n",
    "                for i in reversed(range(1, len(widths) - 1))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Final convolution to map to 3 channels (output image)\n",
    "        self.final_conv = nn.Conv2d(widths[0], 3, kernel_size=1)\n",
    "\n",
    "        # Initialize the conv layers\n",
    "        # initial conv layer\n",
    "        nn.init.xavier_normal_(\n",
    "            self.conv1.weight, gain=nn.init.calculate_gain(\"selu\")\n",
    "        )\n",
    "        if self.conv1.bias is not None:\n",
    "            nn.init.zeros_(self.conv1.bias)\n",
    "        # final conv layer\n",
    "        nn.init.zeros_(self.final_conv.weight)\n",
    "        if self.final_conv.bias is not None:\n",
    "            nn.init.zeros_(self.final_conv.bias)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        noisy_images: torch.Tensor,\n",
    "        noise_variances: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        :param noisy_images torch.Tensor: The noisy images. Shape (batch_size, 3, H, W).\n",
    "        :param noise_variances torch.Tensor: The noise variances. Shape (batch_size, 1, 1, 1).\n",
    "        :return: The denoised images. Shape (batch_size, 3, H, W).\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            noisy_images.shape[2] == noisy_images.shape[3]\n",
    "            and noisy_images.shape[2] == self.image_size\n",
    "        ), f\"Expected image size {self.image_size}, got {noisy_images.shape[2]}\"\n",
    "        # Generate sinusoidal embedding\n",
    "        e = self.sinusoidal_embedding(noise_variances)\n",
    "        e = F.interpolate(e, size=self.image_size, mode=\"nearest\")\n",
    "\n",
    "        # Initial conv layer with the noisy images\n",
    "        x = self.conv1(noisy_images)\n",
    "        x = torch.cat([x, e], dim=1)\n",
    "\n",
    "        # Downsampling blocks\n",
    "        skips = []\n",
    "        for down_block in self.down_blocks:\n",
    "            x = down_block(x, skips)\n",
    "\n",
    "        # Residual blocks in the bottleneck\n",
    "        for block in self.bottleneck:\n",
    "            x = block(x)\n",
    "\n",
    "        # Upsampling blocks\n",
    "        for up_block in self.up_blocks:\n",
    "            x = up_block(x, skips)\n",
    "\n",
    "        # Final convolution to get 3 output channels (image)\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def test_residual_unet():\n",
    "    torch.cuda.set_device(DEVICE_ID)\n",
    "    # Example usage:\n",
    "    model = ResidualUNet(\n",
    "        IMAGE_SIZE, WIDTHS, BLOCK_DEPTH, EMBEDDING_DIM, EMBEDDING_MAX_FREQUENCY\n",
    "    ).cuda()\n",
    "\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"Number of parameters of ResidualUNet is {}\".format(params))\n",
    "\n",
    "    # Input shapes\n",
    "    noisy_images = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE).cuda()\n",
    "    noise_variances = torch.randn(1, 1, 1, 1).cuda()\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(noisy_images, noise_variances)\n",
    "    print(output.shape)  # Should output (1, 3, image_size, image_size)\n",
    "    print(\"Test residual unet done.\")\n",
    "\n",
    "\n",
    "# test_residual_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.nn.parallel import DataParallel as DP\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import get_ipython\n",
    "\n",
    "\n",
    "class DiffusionModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        embedding_dims,\n",
    "        embedding_max_frequency,\n",
    "        widths,\n",
    "        block_depth,\n",
    "        kid_image_size,\n",
    "        kid_diffusion_steps,\n",
    "        plot_diffusion_steps,\n",
    "        min_signal_rate=0.01,\n",
    "        max_signal_rate=0.99,\n",
    "        ema_decay=0.999,\n",
    "        learning_rate=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        mixed_precision=False,\n",
    "        distributed=False,\n",
    "        rank=0,\n",
    "        world_size=1,\n",
    "        device_ids=[0],\n",
    "    ):\n",
    "        self.image_size = image_size\n",
    "        self.widths = widths\n",
    "        self.block_depth = block_depth\n",
    "        self.kid_diffusion_steps = kid_diffusion_steps\n",
    "        self.plot_diffusion_steps = plot_diffusion_steps\n",
    "        self.min_signal_rate = min_signal_rate\n",
    "        self.max_signal_rate = max_signal_rate\n",
    "        self.ema_decay = ema_decay\n",
    "        self.mixed_precision = mixed_precision\n",
    "        self.distributed = distributed\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self.loss_func = nn.L1Loss()\n",
    "\n",
    "        self.mean = (\n",
    "            # torch.tensor([0.485, 0.456, 0.406])\n",
    "            torch.tensor([0.4752, 0.3933, 0.3070])  # precomputed\n",
    "            .view(3, 1, 1)\n",
    "            .requires_grad_(False)\n",
    "        )\n",
    "        self.std = (\n",
    "            # torch.tensor([0.229, 0.224, 0.225])\n",
    "            torch.tensor([0.2902, 0.2372, 0.2679])  # precomputed\n",
    "            .view(3, 1, 1)\n",
    "            .requires_grad_(False)\n",
    "        )\n",
    "        self.network = (\n",
    "            ResidualUNet(\n",
    "                image_size,\n",
    "                widths,\n",
    "                block_depth,\n",
    "                embedding_dims,\n",
    "                embedding_max_frequency,\n",
    "            )\n",
    "            .cuda()\n",
    "            .train()\n",
    "        )\n",
    "        self.ema_network = (\n",
    "            ResidualUNet(\n",
    "                image_size,\n",
    "                widths,\n",
    "                block_depth,\n",
    "                embedding_dims,\n",
    "                embedding_max_frequency,\n",
    "            )\n",
    "            .cuda()\n",
    "            .eval()\n",
    "        )\n",
    "        self.ema_network.load_state_dict(self.network.state_dict())\n",
    "        if self.distributed:\n",
    "            self.network = DDP(self.network, device_ids=[device_ids[rank]])\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.network.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "\n",
    "        # Metrics\n",
    "        self.kid = KID(kid_image_size=kid_image_size).cuda()\n",
    "\n",
    "        # Mixed precision training\n",
    "        self.scaler = GradScaler(\"cuda\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def adapt_normalizer(self, dataloader):\n",
    "        \"\"\"\n",
    "        Called before ddp spawning.\n",
    "        \"\"\"\n",
    "        print(\"Adapting normalizer...\")\n",
    "        # calculate the mean and std of the images for normalization and denormalization\n",
    "        total = torch.zeros(3).cuda()\n",
    "        var = torch.zeros(3).cuda()\n",
    "        if self.distributed:\n",
    "            dataloader.sampler.set_epoch(0)\n",
    "        for images, _ in dataloader:\n",
    "            images = images.cuda()\n",
    "            total += images.sum(dim=(0, 2, 3))\n",
    "        if self.distributed:\n",
    "            dist.all_reduce(total)\n",
    "        mean = total / len(dataloader.dataset) / (IMAGE_SIZE * IMAGE_SIZE)\n",
    "        for images, _ in dataloader:\n",
    "            images = images.cuda()\n",
    "            var += ((images - mean.view(1, 3, 1, 1)) ** 2).sum(dim=(0, 2, 3))\n",
    "        if self.distributed:\n",
    "            dist.all_reduce(var)\n",
    "        var = var / len(dataloader.dataset) / (IMAGE_SIZE * IMAGE_SIZE)\n",
    "        std = torch.sqrt(var)\n",
    "        self.mean = mean.view(3, 1, 1)\n",
    "        self.std = std.view(3, 1, 1)\n",
    "\n",
    "        if self.rank == 0:\n",
    "            print(\"mean: {}\".format(mean))\n",
    "            print(\"std: {}\".format(std))\n",
    "\n",
    "    def normalize(self, images: torch.Tensor):\n",
    "        self.mean = self.mean.cuda()\n",
    "        self.std = self.std.cuda()\n",
    "        # normalize the pixel values to have mean 0 and std 1\n",
    "        images = (images - self.mean) / (self.std + 1e-8)\n",
    "        return images\n",
    "\n",
    "    def denormalize(self, images: torch.Tensor):\n",
    "        self.mean = self.mean.cuda()\n",
    "        self.std = self.std.cuda()\n",
    "        # convert the pixel values back to 0-1 range\n",
    "        images = images * self.std + self.mean\n",
    "        images = torch.clamp(images, 0.0, 1.0)\n",
    "        return images\n",
    "\n",
    "    def diffusion_schedule(self, diffusion_times):\n",
    "        # Diffusion times -> angles\n",
    "        diffusion_times = torch.clamp(diffusion_times, 0.0, 1.0)\n",
    "        max_angle = torch.acos(torch.tensor(self.min_signal_rate))\n",
    "        min_angle = torch.acos(torch.tensor(self.max_signal_rate))\n",
    "\n",
    "        # Angles -> rates\n",
    "        diffusion_angles = (\n",
    "            diffusion_times * (max_angle - min_angle) + min_angle\n",
    "        )\n",
    "        # calculate the noise and signal rates\n",
    "        noise_rates = torch.sin(diffusion_angles)\n",
    "        signal_rates = torch.cos(diffusion_angles)\n",
    "\n",
    "        return noise_rates, signal_rates\n",
    "\n",
    "    def denoise(self, noisy_images, noise_rates, signal_rates, training):\n",
    "        # Predict the noise component and calculate the image component using it\n",
    "        # Here, use the signal_rate and noise_rate to derive the output components\n",
    "        network = self.network if training else self.ema_network\n",
    "        if training:\n",
    "            network.train()\n",
    "        else:\n",
    "            network.eval()\n",
    "        pred_noises = network(noisy_images, noise_rates**2)\n",
    "        pred_images = (noisy_images - pred_noises * noise_rates) / signal_rates\n",
    "\n",
    "        return pred_noises, pred_images\n",
    "\n",
    "    def reverse_diffusion(self, initial_noise, diffusion_steps):\n",
    "        # reverse diffusion = sampling\n",
    "        num_images = initial_noise.shape[0]\n",
    "        step_size = 1.0 / diffusion_steps\n",
    "\n",
    "        # important line:\n",
    "        # at the first sampling step, the \"noisy image\" is pure noise\n",
    "        # but its signal rate is assumed to be nonzero (min_signal_rate)\n",
    "        next_noisy_images = initial_noise\n",
    "        for step in range(diffusion_steps):\n",
    "            # This process gradually reduces the noise to generate a clearer image\n",
    "\n",
    "            # remix the predicted components\n",
    "            # Use the signal_rate and noise_rate from the next step\n",
    "            # to recombine image and noise components\n",
    "            diffusion_times = (\n",
    "                torch.tensor(1 - step * step_size)\n",
    "                .repeat(num_images, 1, 1, 1)\n",
    "                .cuda()\n",
    "            )  # (num_images, 1, 1, 1)\n",
    "            noise_rates, signal_rates = self.diffusion_schedule(\n",
    "                diffusion_times\n",
    "            )  # (num_images, 1, 1, 1), (num_images, 1, 1, 1)\n",
    "            pred_noises, pred_images = self.denoise(\n",
    "                next_noisy_images, noise_rates, signal_rates, training=False\n",
    "            )  # (num_images, 3, H, W), (num_images, 3, H, W)\n",
    "\n",
    "            next_diffusion_times = diffusion_times - step_size\n",
    "            next_noise_rates, next_signal_rates = self.diffusion_schedule(\n",
    "                next_diffusion_times\n",
    "            )  # (num_images, 1, 1, 1), (num_images, 1, 1, 1)\n",
    "\n",
    "            next_noisy_images = (\n",
    "                pred_noises * next_noise_rates\n",
    "                + pred_images * next_signal_rates\n",
    "            )  # (num_images, 3, H, W)\n",
    "\n",
    "        return pred_images\n",
    "\n",
    "    def sample_noise(self, num_images):\n",
    "        noise = torch.randn((num_images, 3, self.image_size, self.image_size))\n",
    "        noise = noise.cuda()\n",
    "        return noise\n",
    "\n",
    "    def generate(self, num_images, diffusion_steps):\n",
    "        # noise -> images -> denormalized images\n",
    "        initial_noise = self.sample_noise(num_images)\n",
    "        generated_images = self.reverse_diffusion(\n",
    "            initial_noise, diffusion_steps\n",
    "        )\n",
    "        generated_images = self.denormalize(generated_images)\n",
    "        return generated_images\n",
    "\n",
    "    def train_step(self, images: torch.Tensor):\n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # normalize images to have standard deviation of 1, like the noises\n",
    "            images = self.normalize(images)  # (batch_size, 3, H, W)\n",
    "            noises = self.sample_noise(batch_size)\n",
    "\n",
    "            # sample uniform random diffusion times\n",
    "            diffusion_times = torch.rand((batch_size, 1, 1, 1)).cuda()\n",
    "            noise_rates, signal_rates = self.diffusion_schedule(\n",
    "                diffusion_times\n",
    "            )\n",
    "            # mix the images with noises accordingly\n",
    "            noisy_images = signal_rates * images + noise_rates * noises\n",
    "\n",
    "        with torch.set_grad_enabled(True), autocast(\n",
    "            \"cuda\", dtype=torch.float16, enabled=self.mixed_precision\n",
    "        ):\n",
    "            # predict the noise and image components\n",
    "            pred_noises, pred_images = self.denoise(\n",
    "                noisy_images, noise_rates, signal_rates, training=True\n",
    "            )\n",
    "            noise_loss = self.loss_func(\n",
    "                pred_noises, noises\n",
    "            )  # used for training\n",
    "            image_loss = self.loss_func(\n",
    "                pred_images, images\n",
    "            )  # only used as metric\n",
    "\n",
    "        self.optimizer.zero_grad(set_to_none=True)\n",
    "        if self.mixed_precision:\n",
    "            self.scaler.scale(noise_loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.network.parameters(), max_norm=1.0\n",
    "            )\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "        else:\n",
    "            noise_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.network.parameters(), max_norm=1.0\n",
    "            )\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.ema_update()\n",
    "\n",
    "        return {\n",
    "            \"n_loss\": noise_loss.item(),\n",
    "            \"i_loss\": image_loss.item(),\n",
    "        }\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ema_update(self):\n",
    "        # Update the EMA network with the main network's parameters\n",
    "        for param, ema_param in zip(\n",
    "            self.network.module.parameters(), self.ema_network.parameters()\n",
    "        ):\n",
    "            ema_param.data.mul_(self.ema_decay).add_(\n",
    "                param.data, alpha=1.0 - self.ema_decay\n",
    "            )\n",
    "\n",
    "    def test_step(self, images):\n",
    "        with torch.no_grad():\n",
    "            batch_size = images.shape[0]\n",
    "\n",
    "            # normalize images to have standard deviation of 1, like the noises\n",
    "            images = self.normalize(images)\n",
    "            noises = self.sample_noise(batch_size)\n",
    "\n",
    "            # sample uniform random diffusion times\n",
    "            diffusion_times = torch.rand((batch_size, 1, 1, 1)).cuda()\n",
    "            noise_rates, signal_rates = self.diffusion_schedule(\n",
    "                diffusion_times\n",
    "            )\n",
    "            # mix the images with noises accordingly\n",
    "            noisy_images = signal_rates * images + noise_rates * noises\n",
    "\n",
    "            with autocast(\"cuda\", enabled=self.mixed_precision):\n",
    "                pred_noises, pred_images = self.denoise(\n",
    "                    noisy_images, noise_rates, signal_rates, training=False\n",
    "                )\n",
    "\n",
    "                noise_loss = self.loss_func(pred_noises, noises)\n",
    "                image_loss = self.loss_func(pred_images, images)\n",
    "\n",
    "            # measure KID between real and generated images\n",
    "            # this is computationally demanding, kid_diffusion_steps has to be small\n",
    "            images = self.denormalize(images)\n",
    "            generated_images = self.generate(\n",
    "                num_images=batch_size,\n",
    "                diffusion_steps=self.kid_diffusion_steps,\n",
    "            )\n",
    "            self.kid.reset()\n",
    "            self.kid.update(generated_images, images)\n",
    "\n",
    "            return {\n",
    "                \"n_loss\": noise_loss.item(),\n",
    "                \"i_loss\": image_loss.item(),\n",
    "                \"kid\": self.kid.compute().item(),\n",
    "            }\n",
    "\n",
    "    def plot_images(\n",
    "        self,\n",
    "        num_rows=3,\n",
    "        num_cols=6,\n",
    "        save=False,\n",
    "        output_dir=\"outputs\",\n",
    "        epoch=None,\n",
    "    ):\n",
    "        ipy = get_ipython()\n",
    "        if ipy is None and not save:\n",
    "            return\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_images = self.generate(\n",
    "                num_images=num_rows * num_cols,\n",
    "                diffusion_steps=self.plot_diffusion_steps,\n",
    "            )\n",
    "\n",
    "        plt.figure(figsize=(num_cols * 2.0, num_rows * 2.0))\n",
    "        for row in range(num_rows):\n",
    "            for col in range(num_cols):\n",
    "                idx = row * num_cols + col\n",
    "                plt.subplot(num_rows, num_cols, idx + 1)\n",
    "                plt.imshow(\n",
    "                    generated_images[idx]\n",
    "                    .detach()\n",
    "                    .cpu()\n",
    "                    .permute(1, 2, 0)\n",
    "                    .numpy()\n",
    "                )\n",
    "                plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        if ipy is not None:\n",
    "            plt.show()\n",
    "        if save:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            plt.savefig(\n",
    "                os.path.join(\n",
    "                    output_dir,\n",
    "                    \"img{}.png\".format(\n",
    "                        \"\" if epoch is None else \"_epoch{}\".format(epoch)\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "            print(\"Saved generated images at\", output_dir)\n",
    "        plt.close()\n",
    "\n",
    "    def get_model_state_dict(self):\n",
    "        if self.distributed:\n",
    "            network = self.network.module\n",
    "        else:\n",
    "            network = self.network\n",
    "        ema_network = self.ema_network\n",
    "        return {\n",
    "            \"network\": network.state_dict(),\n",
    "            \"ema_network\": ema_network.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "    def set_model_state_dict(self, state_dict):\n",
    "        if self.distributed:\n",
    "            self.network.module.load_state_dict(state_dict[\"network\"])\n",
    "        else:\n",
    "            self.network.load_state_dict(state_dict[\"network\"])\n",
    "        self.ema_network.load_state_dict(state_dict[\"ema_network\"])\n",
    "        self.optimizer.load_state_dict(state_dict[\"optimizer\"])\n",
    "\n",
    "\n",
    "def test_model():\n",
    "    torch.cuda.set_device(DEVICE_ID)\n",
    "    model = DiffusionModel(\n",
    "        image_size=IMAGE_SIZE,\n",
    "        embedding_dims=EMBEDDING_DIM,\n",
    "        embedding_max_frequency=EMBEDDING_MAX_FREQUENCY,\n",
    "        widths=WIDTHS,\n",
    "        block_depth=BLOCK_DEPTH,\n",
    "        kid_image_size=KID_IMAGE_SIZE,\n",
    "        kid_diffusion_steps=KID_DIFFUSION_STEPS,\n",
    "        plot_diffusion_steps=PLOT_DIFFUSION_STEPS,\n",
    "        min_signal_rate=MIN_SIGNAL_RATE,\n",
    "        max_signal_rate=MAX_SIGNAL_RATE,\n",
    "        ema_decay=EMA,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        mixed_precision=MIXED_PRECISION,\n",
    "    )\n",
    "    model.setup()\n",
    "\n",
    "    # Test\n",
    "    test_loader = get_dataloader(\n",
    "        batch_size=BATCH_SIZE, image_size=IMAGE_SIZE, split=\"test\"\n",
    "    )\n",
    "    images, _ = next(iter(test_loader))\n",
    "    images = images.cuda()\n",
    "\n",
    "    model.adapt_normalizer(test_loader)\n",
    "\n",
    "    start_ts = perf_counter()\n",
    "    train_metrics = model.train_step(images)\n",
    "    print(\"train metrics:\", train_metrics)\n",
    "    print(\"train time:\", perf_counter() - start_ts)\n",
    "\n",
    "    start_ts = perf_counter()\n",
    "    test_metrics = model.test_step(images)\n",
    "    print(\"test metrics:\", test_metrics)\n",
    "    print(\"test time:\", perf_counter() - start_ts)\n",
    "\n",
    "    start_ts = perf_counter()\n",
    "    model.plot_images(num_rows=1)\n",
    "    print(\"plot time:\", perf_counter() - start_ts)\n",
    "    print(\"Test model done.\")\n",
    "\n",
    "\n",
    "# test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "def save_checkpoint(epoch, model_state_dict, checkpoint_dir, checkpoint_name):\n",
    "    \"\"\"\n",
    "    Save a checkpoint to a specified directory.\n",
    "\n",
    "    :param epoch: The epoch number to save in the checkpoint.\n",
    "    :param model: The model to save in the checkpoint.\n",
    "    :param checkpoint_dir: The directory to save the checkpoint in.\n",
    "    :param checkpoint_name: The name of the checkpoint file, will be appended with the epoch number.\n",
    "    \"\"\"\n",
    "    checkpoint_path = os.path.join(\n",
    "        checkpoint_dir, f\"{checkpoint_name}_{epoch:03d}.pt\"\n",
    "    )\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    torch.save(\n",
    "        model_state_dict,\n",
    "        checkpoint_path,\n",
    "    )\n",
    "    print(f\"Saved checkpoint for epoch {epoch} at {checkpoint_path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "    checkpoint_dir,\n",
    "    checkpoint_name,\n",
    "    epoch=None,\n",
    "    device=None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Load a checkpoint from a specified directory.\n",
    "\n",
    "    :param model: The model to load the checkpoint into.\n",
    "    :param checkpoint_dir: The directory to search for the checkpoint.\n",
    "    :param checkpoint_name: The name of the checkpoint file, will be appended with the epoch number.\n",
    "    :param epoch: The epoch to load the checkpoint from. If None, the latest checkpoint is loaded.\n",
    "    If no checkpoint is found, the function prints a message and returns.\n",
    "    \"\"\"\n",
    "    if epoch is None:\n",
    "        # Search for the latest checkpoint\n",
    "        files = os.listdir(checkpoint_dir)\n",
    "        files = [\n",
    "            file\n",
    "            for file in files\n",
    "            if file.startswith(checkpoint_name) and file.endswith(\".pt\")\n",
    "        ]\n",
    "        if not files:\n",
    "            print(\"No checkpoint found.\")\n",
    "            return None\n",
    "        files.sort()\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, files[-1])\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(\n",
    "            checkpoint_dir, f\"{checkpoint_name}_{epoch:03d}.pt\"\n",
    "        )\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"Checkpoint for epoch {epoch} not found.\")\n",
    "        return None\n",
    "    model_state_dict = torch.load(\n",
    "        checkpoint_path, weights_only=False, map_location=device\n",
    "    )\n",
    "    return model_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "from datetime import timedelta\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "def find_free_port():\n",
    "    \"\"\"\n",
    "    Find a free port on the local machine.\n",
    "    \"\"\"\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.bind((\"\", 0))\n",
    "        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "        return str(s.getsockname()[1])\n",
    "\n",
    "\n",
    "def setup(rank, world_size, device_ids=[0]):\n",
    "    \"\"\"Setup the distributed process group.\"\"\"\n",
    "    # timeout = timedelta(seconds=30)\n",
    "    timeout = timedelta(minutes=10)\n",
    "    torch.cuda.set_device(device_ids[rank])\n",
    "    dist.init_process_group(\n",
    "        \"nccl\", timeout=timeout, rank=rank, world_size=world_size\n",
    "    )\n",
    "    print(\"Setupped process group for rank\", rank)\n",
    "\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Clean up the distributed process group.\"\"\"\n",
    "    dist.destroy_process_group()\n",
    "    print(\"Cleaned up process group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "\n",
    "if EPOCHS > 0:\n",
    "    if DISTRIBUTED:\n",
    "        setup(RANK, WORLD_SIZE, device_ids=DEVICE_IDS)\n",
    "        device = torch.device(f\"cuda\")\n",
    "    else:\n",
    "        device = (\n",
    "            torch.device(DEVICE_ID)\n",
    "            if DEVICE_ID is not None\n",
    "            else torch.device(\"cpu\")\n",
    "        )\n",
    "\n",
    "    model = DiffusionModel(\n",
    "        image_size=IMAGE_SIZE,\n",
    "        embedding_dims=EMBEDDING_DIM,\n",
    "        embedding_max_frequency=EMBEDDING_MAX_FREQUENCY,\n",
    "        widths=WIDTHS,\n",
    "        block_depth=BLOCK_DEPTH,\n",
    "        kid_image_size=KID_IMAGE_SIZE,\n",
    "        kid_diffusion_steps=KID_DIFFUSION_STEPS,\n",
    "        plot_diffusion_steps=PLOT_DIFFUSION_STEPS,\n",
    "        min_signal_rate=MIN_SIGNAL_RATE,\n",
    "        max_signal_rate=MAX_SIGNAL_RATE,\n",
    "        ema_decay=EMA,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        mixed_precision=MIXED_PRECISION,\n",
    "        distributed=DISTRIBUTED,\n",
    "        rank=RANK,\n",
    "        world_size=WORLD_SIZE,\n",
    "        device_ids=DEVICE_IDS,\n",
    "    )\n",
    "\n",
    "    model_state_dict = load_checkpoint(\n",
    "        CHECKPOINT_DIR,\n",
    "        CHECKPOINT_NAME,\n",
    "        epoch=START_EPOCH,\n",
    "        device=device,\n",
    "    )\n",
    "    if model_state_dict is not None:\n",
    "        model.set_model_state_dict(model_state_dict)\n",
    "\n",
    "    # Get the data loader\n",
    "    train_loader = get_dataloader(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        split=\"train\",\n",
    "        repeats=DATASET_REPETITIONS,\n",
    "        shuffle=True,\n",
    "        rank=RANK,\n",
    "        distributed=DISTRIBUTED,\n",
    "    )\n",
    "    val_loader = get_dataloader(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        split=\"val\",\n",
    "        rank=RANK,\n",
    "        distributed=DISTRIBUTED,\n",
    "    )\n",
    "\n",
    "    for epoch in range(START_EPOCH + 1, EPOCHS + START_EPOCH + 1):\n",
    "        print(\n",
    "            \"{}, epoch {:3d}/{:3d}\".format(\n",
    "                datetime.now(), epoch, EPOCHS + START_EPOCH\n",
    "            )\n",
    "        )\n",
    "        start_ts = time.perf_counter()\n",
    "        epoch_train_metrics = torch.zeros(2).cuda()\n",
    "        epoch_val_metrics = torch.zeros(3).cuda()\n",
    "\n",
    "        # Training\n",
    "        if DISTRIBUTED:\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "        for _idx, (images, _) in enumerate(train_loader):\n",
    "            idx = _idx + 1\n",
    "            images = images.cuda()\n",
    "            train_metrics = model.train_step(images)\n",
    "            epoch_train_metrics += torch.tensor(\n",
    "                [train_metrics[\"n_loss\"], train_metrics[\"i_loss\"]]\n",
    "            ).cuda()\n",
    "            if idx % 20 == 0:\n",
    "                print(\n",
    "                    \"rank {:2d}, train epoch {:3d}/{:3d}, batch {:4d}/{:4d}, n_loss: {:.4f}, i_loss: {:.4f}\".format(\n",
    "                        RANK,\n",
    "                        epoch,\n",
    "                        EPOCHS + START_EPOCH,\n",
    "                        idx,\n",
    "                        len(train_loader),\n",
    "                        train_metrics[\"n_loss\"],\n",
    "                        train_metrics[\"i_loss\"],\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Save checkpoint\n",
    "        if RANK == 0:\n",
    "            save_checkpoint(\n",
    "                epoch,\n",
    "                model.get_model_state_dict(),\n",
    "                CHECKPOINT_DIR,\n",
    "                CHECKPOINT_NAME,\n",
    "            )\n",
    "\n",
    "        # Testing\n",
    "        for idx, (images, _) in enumerate(val_loader):\n",
    "            images = images.cuda()\n",
    "            test_metrics = model.test_step(images)\n",
    "            epoch_val_metrics += torch.tensor(\n",
    "                [\n",
    "                    test_metrics[\"n_loss\"],\n",
    "                    test_metrics[\"i_loss\"],\n",
    "                    test_metrics[\"kid\"],\n",
    "                ]\n",
    "            ).cuda()\n",
    "\n",
    "        if DISTRIBUTED:\n",
    "            dist.all_reduce(epoch_train_metrics, op=dist.ReduceOp.AVG)\n",
    "            dist.all_reduce(epoch_val_metrics, op=dist.ReduceOp.AVG)\n",
    "\n",
    "        # Print metrics\n",
    "        if RANK == 0:\n",
    "            avg_train_metrics = epoch_train_metrics / len(train_loader)\n",
    "            avg_val_metrics = epoch_val_metrics / len(val_loader)\n",
    "            epoch_train_metrics.zero_()\n",
    "            epoch_val_metrics.zero_()\n",
    "            print(\n",
    "                \"rank {:2d}, epoch {:3d}/{:3d}, n_loss: {:.4f}, i_loss: {:.4f}, val n_loss: {:.4f}, val i_loss: {:.4f}, val kid: {:.4f}, took {:.2f}s\".format(\n",
    "                    RANK,\n",
    "                    epoch,\n",
    "                    EPOCHS + START_EPOCH,\n",
    "                    avg_train_metrics[0],\n",
    "                    avg_train_metrics[1],\n",
    "                    avg_val_metrics[0],\n",
    "                    avg_val_metrics[1],\n",
    "                    avg_val_metrics[2],\n",
    "                    time.perf_counter() - start_ts,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if RANK == 0 and epoch % PLOT_EVERY == 0:\n",
    "            model.plot_images(\n",
    "                save=SAVE_PLOTS,\n",
    "                output_dir=OUTPUT_DIR,\n",
    "                epoch=epoch,\n",
    "            )\n",
    "\n",
    "        if DISTRIBUTED:\n",
    "            dist.barrier()\n",
    "\n",
    "    if DISTRIBUTED:\n",
    "        cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
