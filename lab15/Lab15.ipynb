{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import moviepy.editor as mpy\n",
    "import skimage.transform\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.losses as kls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the fourth GPU\n",
    "        tf.config.set_visible_devices(gpus[0], \"GPU\")\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "        logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = (\n",
    "    \"dummy\"  # this line make pop-out window not appear\n",
    ")\n",
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "\n",
    "game = FlappyBird()\n",
    "env = PLE(game, fps=30, display_screen=False)  # environment interface to game\n",
    "env.reset_game()\n",
    "\n",
    "test_game = FlappyBird()\n",
    "test_env = PLE(test_game, fps=30, display_screen=False)\n",
    "test_env.reset_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./movie_f\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparas = {\n",
    "    \"image_size\": 84,\n",
    "    \"num_stack\": 4,\n",
    "    \"action_dim\": len(env.getActionSet()),\n",
    "    \"hidden_size\": 256,\n",
    "    \"lr\": 0.0001,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lambda\": 0.95,\n",
    "    \"clip_val\": 0.2,\n",
    "    \"ppo_epochs\": 8,\n",
    "    \"test_epochs\": 1,\n",
    "    \"num_steps\": 512,\n",
    "    \"mini_batch_size\": 64,\n",
    "    \"target_reward\": 200,\n",
    "    \"max_episode\": 30000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not modify this method\n",
    "def make_anim(images, fps=60, true_image=False):\n",
    "    duration = len(images) / fps\n",
    "\n",
    "    def make_frame(t):\n",
    "        try:\n",
    "            x = images[int(len(images) / duration * t)]\n",
    "        except:\n",
    "            x = images[-1]\n",
    "\n",
    "        if true_image:\n",
    "            return x.astype(np.uint8)\n",
    "        else:\n",
    "            return ((x + 1) / 2 * 255).astype(np.uint8)\n",
    "\n",
    "    clip = mpy.VideoClip(make_frame, duration=duration)\n",
    "    clip.fps = fps\n",
    "\n",
    "    return clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_screen(screen):\n",
    "    screen = skimage.transform.rotate(screen, -90, resize=True)\n",
    "    screen = screen[:400, :]\n",
    "    screen = skimage.transform.resize(\n",
    "        screen, [hparas[\"image_size\"], hparas[\"image_size\"], 1]\n",
    "    )\n",
    "    return screen.astype(np.float32)\n",
    "\n",
    "\n",
    "def frames_to_state(input_frames):\n",
    "    if len(input_frames) == 1:\n",
    "        state = np.concatenate(input_frames * 4, axis=-1)\n",
    "    elif len(input_frames) == 2:\n",
    "        state = np.concatenate(\n",
    "            input_frames[0:1] * 2 + input_frames[1:] * 2, axis=-1\n",
    "        )\n",
    "    elif len(input_frames) == 3:\n",
    "        state = np.concatenate(input_frames + input_frames[2:], axis=-1)\n",
    "    else:\n",
    "        state = np.concatenate(input_frames[-4:], axis=-1)\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(tf.keras.Model):\n",
    "    def __init__(self, hparas):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extractor = tf.keras.Sequential(\n",
    "            [\n",
    "                # Convolutional Layers\n",
    "                tf.keras.layers.Conv2D(filters=32, kernel_size=8, strides=4),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.Conv2D(filters=64, kernel_size=4, strides=2),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1),\n",
    "                tf.keras.layers.ReLU(),\n",
    "                # Embedding Layers\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(hparas[\"hidden_size\"]),\n",
    "                tf.keras.layers.ReLU(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Actor Network\n",
    "        self.actor = tf.keras.layers.Dense(\n",
    "            hparas[\"action_dim\"], activation=\"softmax\"\n",
    "        )\n",
    "        # Critic Network\n",
    "        self.critic = tf.keras.layers.Dense(1, activation=None)\n",
    "\n",
    "    def call(self, input):\n",
    "        x = self.feature_extractor(input)\n",
    "        action_logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return action_logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, hparas):\n",
    "        self.gamma = hparas[\"gamma\"]\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=hparas[\"lr\"])\n",
    "        self.actor_critic = ActorCriticNetwork(hparas)\n",
    "        self.clip_pram = hparas[\"clip_val\"]\n",
    "\n",
    "    def ppo_iter(\n",
    "        self, mini_batch_size, states, actions, log_probs, returns, advantage\n",
    "    ):\n",
    "        batch_size = states.shape[0]\n",
    "        for _ in range(batch_size // mini_batch_size):\n",
    "            rand_ids = tf.convert_to_tensor(\n",
    "                np.random.randint(0, batch_size, mini_batch_size),\n",
    "                dtype=tf.int32,\n",
    "            )\n",
    "            yield tf.gather(states, rand_ids), tf.gather(\n",
    "                actions, rand_ids\n",
    "            ), tf.gather(log_probs, rand_ids), tf.gather(\n",
    "                returns, rand_ids\n",
    "            ), tf.gather(\n",
    "                advantage, rand_ids\n",
    "            )\n",
    "\n",
    "    def ppo_update(\n",
    "        self,\n",
    "        ppo_epochs,\n",
    "        mini_batch_size,\n",
    "        states,\n",
    "        actions,\n",
    "        log_probs,\n",
    "        discount_rewards,\n",
    "        advantages,\n",
    "    ):\n",
    "        total_actor_loss = 0\n",
    "        total_critic_loss = 0\n",
    "        for _ in range(ppo_epochs):\n",
    "            for (\n",
    "                state,\n",
    "                action,\n",
    "                old_log_probs,\n",
    "                reward,\n",
    "                advantage,\n",
    "            ) in self.ppo_iter(\n",
    "                mini_batch_size,\n",
    "                states,\n",
    "                actions,\n",
    "                log_probs,\n",
    "                discount_rewards,\n",
    "                advantages,\n",
    "            ):\n",
    "                reward = tf.expand_dims(reward, axis=-1)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    prob, value = self.actor_critic(state, training=True)\n",
    "                    dist = tfp.distributions.Categorical(\n",
    "                        probs=prob, dtype=tf.float32\n",
    "                    )\n",
    "                    entropy = tf.math.reduce_mean(dist.entropy())\n",
    "                    new_log_probs = dist.log_prob(action)\n",
    "\n",
    "                    # PPO ratio\n",
    "                    ratio = tf.math.exp(new_log_probs - old_log_probs)\n",
    "                    surr1 = ratio * advantage\n",
    "                    surr2 = (\n",
    "                        tf.clip_by_value(\n",
    "                            ratio, 1.0 - self.clip_pram, 1.0 + self.clip_pram\n",
    "                        )\n",
    "                        * advantage\n",
    "                    )\n",
    "\n",
    "                    actor_loss = (\n",
    "                        tf.math.negative(\n",
    "                            tf.math.reduce_mean(tf.math.minimum(surr1, surr2))\n",
    "                        )\n",
    "                        - 0.1 * entropy\n",
    "                    )\n",
    "                    critic_loss = 0.5 * tf.math.reduce_mean(\n",
    "                        kls.mean_squared_error(reward, value)\n",
    "                    )\n",
    "\n",
    "                    total_loss = actor_loss + critic_loss\n",
    "\n",
    "                # single optimizer\n",
    "                grads = tape.gradient(\n",
    "                    total_loss, self.actor_critic.trainable_variables\n",
    "                )\n",
    "                self.optimizer.apply_gradients(\n",
    "                    zip(grads, self.actor_critic.trainable_variables)\n",
    "                )\n",
    "\n",
    "                total_actor_loss += actor_loss\n",
    "                total_critic_loss += critic_loss\n",
    "        return total_actor_loss, total_critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1506.02438.pdf\n",
    "# Equation 16\n",
    "def compute_gae(rewards, masks, values, gamma, LAMBDA):\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + gamma * values[i + 1] * masks[i] - values[i]\n",
    "        gae = delta + gamma * LAMBDA * masks[i] * gae\n",
    "        returns.append(gae + values[i])\n",
    "\n",
    "    returns.reverse()\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reward(test_env, agent):\n",
    "    total_reward = 0\n",
    "    # Reset the environment\n",
    "    test_env.reset_game()\n",
    "    input_frames = [preprocess_screen(test_env.getScreenGrayscale())]\n",
    "\n",
    "    while not test_env.game_over():\n",
    "\n",
    "        state = frames_to_state(input_frames)\n",
    "        state = tf.expand_dims(state, axis=0)\n",
    "        prob, value = agent.actor_critic(state)\n",
    "\n",
    "        action = np.argmax(prob[0].numpy())\n",
    "        reward = test_env.act(test_env.getActionSet()[action])\n",
    "        total_reward += reward\n",
    "\n",
    "        input_frames.append(preprocess_screen(test_env.getScreenGrayscale()))\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(hparas)\n",
    "max_episode = hparas[\"max_episode\"]\n",
    "test_per_n_episode = 10\n",
    "force_save_per_n_episode = 1000\n",
    "early_stop_reward = 10\n",
    "\n",
    "start_s = 0\n",
    "best_reward = -5.0\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    actor_critic=agent.actor_critic,\n",
    "    optimizer=agent.optimizer,\n",
    ")\n",
    "\n",
    "# Load from old checkpoint\n",
    "# checkpoint.restore('ckpt_dir/ckpt-?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_reward = []\n",
    "total_avgr = []\n",
    "early_stop = False\n",
    "avg_rewards_list = []\n",
    "\n",
    "env.reset_game()\n",
    "\n",
    "for s in range(0, max_episode):\n",
    "    if early_stop == True:\n",
    "        break\n",
    "\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    masks = []\n",
    "    values = []\n",
    "\n",
    "    display_frames = [env.getScreenRGB()]\n",
    "    input_frames = [preprocess_screen(env.getScreenGrayscale())]\n",
    "\n",
    "    for step in range(hparas[\"num_steps\"]):\n",
    "\n",
    "        state = frames_to_state(input_frames)\n",
    "        state = tf.expand_dims(state, axis=0)\n",
    "        prob, value = agent.actor_critic(state)\n",
    "\n",
    "        dist = tfp.distributions.Categorical(probs=prob[0], dtype=tf.float32)\n",
    "        action = dist.sample(1)\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        reward = env.act(env.getActionSet()[int(action.numpy())])\n",
    "\n",
    "        done = env.game_over()\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        values.append(value[0])\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(tf.convert_to_tensor(reward, dtype=tf.float32))\n",
    "        masks.append(tf.convert_to_tensor(1 - int(done), dtype=tf.float32))\n",
    "\n",
    "        display_frames.append(env.getScreenRGB())\n",
    "        input_frames.append(preprocess_screen(env.getScreenGrayscale()))\n",
    "\n",
    "        if done:\n",
    "            env.reset_game()\n",
    "            input_frames = [preprocess_screen(env.getScreenGrayscale())]\n",
    "\n",
    "    _, next_value = agent.actor_critic(state)\n",
    "    values.append(next_value[0])\n",
    "\n",
    "    returns = compute_gae(\n",
    "        rewards, masks, values, hparas[\"gamma\"], hparas[\"lambda\"]\n",
    "    )\n",
    "\n",
    "    returns = tf.concat(returns, axis=0)\n",
    "    log_probs = tf.concat(log_probs, axis=0)\n",
    "    values = tf.concat(values, axis=0)\n",
    "    states = tf.concat(states, axis=0)\n",
    "    actions = tf.concat(actions, axis=0)\n",
    "    advantage = returns - values[:-1]\n",
    "\n",
    "    a_loss, c_loss = agent.ppo_update(\n",
    "        hparas[\"ppo_epochs\"],\n",
    "        hparas[\"mini_batch_size\"],\n",
    "        states,\n",
    "        actions,\n",
    "        log_probs,\n",
    "        returns,\n",
    "        advantage,\n",
    "    )\n",
    "    print(\n",
    "        \"[Episode %d]  Actor loss: %.5f, Critic loss: %.5f\"\n",
    "        % (s, a_loss, c_loss)\n",
    "    )\n",
    "\n",
    "    if s % test_per_n_episode == 0:\n",
    "        # test agent hparas['test_epochs'] times to get the average reward\n",
    "        avg_reward = np.mean(\n",
    "            [\n",
    "                test_reward(test_env, agent)\n",
    "                for _ in range(hparas[\"test_epochs\"])\n",
    "            ]\n",
    "        )\n",
    "        print(\n",
    "            \"Test average reward is %.1f, Current best average reward is %.1f\\n\"\n",
    "            % (avg_reward, best_reward)\n",
    "        )\n",
    "        avg_rewards_list.append(avg_reward)\n",
    "\n",
    "        if avg_reward > best_reward:\n",
    "            best_reward = avg_reward\n",
    "            agent.actor_critic.save(\n",
    "                \"./save/Actor/model_actor_{}_{}\".format(s, avg_reward),\n",
    "                save_format=\"tf\",\n",
    "            )\n",
    "            checkpoint.save(file_prefix=\"./save/checkpoints/ckpt\")\n",
    "\n",
    "    if s % force_save_per_n_episode == 0:\n",
    "        agent.actor_critic.save(\n",
    "            \"./save/Actor/model_actor_{}_{}\".format(s, avg_reward),\n",
    "            save_format=\"tf\",\n",
    "        )\n",
    "        checkpoint.save(file_prefix=\"./save/checkpoints/ckpt\")\n",
    "        clip = make_anim(display_frames, fps=60, true_image=True).rotate(-90)\n",
    "        clip.write_videofile(\n",
    "            \"movie_f/{}_demo-{}.webm\".format(\"Lab15\", s), fps=60\n",
    "        )\n",
    "        display(\n",
    "            clip.ipython_display(fps=60, autoplay=1, loop=1, maxduration=120)\n",
    "        )\n",
    "\n",
    "    if best_reward >= early_stop_reward:\n",
    "        early_stop = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-lab15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
