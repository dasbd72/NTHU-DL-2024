{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De-captcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TODO: change ID\n",
    "ID = \"attn-v5\"\n",
    "\n",
    "# TODO: change device\n",
    "DEVICE = \"cuda:0\"  # \"cuda:i\" or \"cpu\"\n",
    "OMP_NUM_THREADS = 10\n",
    "SEED = 42\n",
    "\n",
    "TRAIN_TEST_PIVOT = 100000  # 0 - 100000 for train, 100000 - 120000 for test\n",
    "DATASET_PATH = \"./words_captcha/spec_train_val.txt\"\n",
    "IMAGE_DIR = \"./words_captcha/\"\n",
    "\n",
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 10\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "\n",
    "# TODO: change epochs\n",
    "START_EPOCH = 0\n",
    "EPOCHS = 0\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "CHECKPOINT_DIR = os.path.join(\"./ckpts/\", ID)\n",
    "CHECKPOINT_NAME = \"ckpt\"\n",
    "CHECKPOINT_NAME_ENC = \"encoder\"\n",
    "CHECKPOINT_NAME_DEC = \"decoder\"\n",
    "OUTPUT_DIR = os.path.join(\"./outputs/\", ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 4\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {gpus}\")\n",
    "    device = torch.device(DEVICE)\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(OMP_NUM_THREADS)\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def load_labels(path: str):\n",
    "    assert os.path.exists(path)\n",
    "    labels: list[tuple[str, str]] = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            parts = line.split(\" \")\n",
    "            assert len(parts) == 2, f\"Invalid line at {idx}: {line}\"\n",
    "            image_name, label = parts\n",
    "            labels.append((image_name, label))\n",
    "    return labels\n",
    "\n",
    "\n",
    "class DatasetGenerator(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_dir: str,\n",
    "        image_size: int,\n",
    "        labels: list[tuple[str, str]],\n",
    "        augmentation: bool = False,\n",
    "        target_char_count: int = 20000,\n",
    "        max_dataset_size: int = 500000,\n",
    "        return_name: bool = False,\n",
    "        verbose: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param image_dir str: Directory containing images\n",
    "        :param image_size int: Size of the image\n",
    "        :param labels list[tuple[str, str]]: List of tuples containing image name and label\n",
    "        :param augmentation bool: Whether to apply train-time augmentations\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.image_size = image_size\n",
    "        self.return_name = return_name\n",
    "        self.labels = labels.copy()\n",
    "\n",
    "        if augmentation:\n",
    "            # Augment the dataset to have at least target_char_count of each character\n",
    "            # Set up data structures\n",
    "            char_list: list[str] = []\n",
    "            char_freq: dict[str, int] = {}\n",
    "            char_images: dict[str, list[str]] = {}\n",
    "            image_labels: dict[str, str] = {}\n",
    "            # Count appearances of each character\n",
    "            for image_name, label in labels:\n",
    "                image_labels[image_name] = label\n",
    "                for char in label:\n",
    "                    char_freq[char] = char_freq.get(char, 0) + 1\n",
    "            # Set up character list and image count\n",
    "            for char in char_freq.keys():\n",
    "                char_list.append(char)\n",
    "                char_images[char] = set()\n",
    "            char_list = sorted(char_list)\n",
    "            for image_name, label in labels:\n",
    "                for char in label:\n",
    "                    char_images[char].add(image_name)\n",
    "            for char in char_freq.keys():\n",
    "                char_images[char] = list(char_images[char])\n",
    "            if verbose:\n",
    "                print(f\"Character frequencies: {char_freq}\")\n",
    "                print(f\"Dataset size: {len(labels)}\")\n",
    "\n",
    "            # Start augmenting the dataset until all characters have been seen at least target_char_count times or max_dataset_size is reached\n",
    "            while len(self.labels) < max_dataset_size:\n",
    "                if all(\n",
    "                    freq >= target_char_count for freq in char_freq.values()\n",
    "                ):\n",
    "                    # All characters have been seen at least target_char_count times\n",
    "                    break\n",
    "                # Select image of lowest frequency character\n",
    "                top_char_freq = sorted(\n",
    "                    char_freq.items(), key=lambda x: x[1], reverse=True\n",
    "                )\n",
    "                top_char = [char for char, _ in top_char_freq]\n",
    "                candidate_images = char_images[top_char[-1]]\n",
    "                image_name = random.choice(candidate_images)\n",
    "                label = image_labels[image_name]\n",
    "                # Skip the image if it contains the top frequency character\n",
    "                if any(char in top_char[:3] for char in label):\n",
    "                    continue\n",
    "\n",
    "                # Add the image to the dataset\n",
    "                self.labels.append((image_name, label))\n",
    "                # Update character frequency\n",
    "                for char in label:\n",
    "                    char_freq[char] += 1\n",
    "\n",
    "            image_freq = {}\n",
    "            for image_name, label in self.labels:\n",
    "                image_freq[image_name] = image_freq.get(image_name, 0) + 1\n",
    "            if verbose:\n",
    "                print(f\"Modified character frequencies: {char_freq}\")\n",
    "                print(f\"Modified dataset size: {len(self.labels)}\")\n",
    "                print(\n",
    "                    f\"Top Image Frequencies: {sorted(image_freq.items(), key=lambda x: x[1], reverse=True)[:10]}\"\n",
    "                )\n",
    "\n",
    "            # Define image transformations\n",
    "            appearance_transforms = [\n",
    "                A.Sharpen(alpha=(0.1, 0.4), lightness=(0.1, 0.4), p=0.2),\n",
    "                A.SomeOf(\n",
    "                    [\n",
    "                        A.RGBShift(\n",
    "                            r_shift_limit=(-13, 13),\n",
    "                            g_shift_limit=(-13, 13),\n",
    "                            b_shift_limit=(-13, 13),\n",
    "                            p=1,\n",
    "                        ),\n",
    "                        A.RandomBrightnessContrast(p=1),\n",
    "                        A.RandomGamma(p=1),\n",
    "                    ],\n",
    "                    n=1,\n",
    "                    p=1,\n",
    "                ),\n",
    "            ]\n",
    "            self.transfom = A.Compose(\n",
    "                [\n",
    "                    A.Resize(image_size, image_size),\n",
    "                    *appearance_transforms,\n",
    "                    A.Normalize(\n",
    "                        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                    ),  # Rescale [0,1] to [-1,1]\n",
    "                    ToTensorV2(),\n",
    "                ],\n",
    "            )\n",
    "        else:\n",
    "            self.transfom = A.Compose(\n",
    "                [\n",
    "                    A.Resize(image_size, image_size),\n",
    "                    A.Normalize(\n",
    "                        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                    ),  # Rescale [0,1] to [-1,1]\n",
    "                    ToTensorV2(),\n",
    "                ],\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name, label = self.labels[idx]\n",
    "        image = cv2.imread(self.image_dir + image_name + \".png\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Apply transformations\n",
    "        transformed = self.transfom(image=image)\n",
    "        image = transformed[\"image\"]\n",
    "        if self.return_name:\n",
    "            return image, label, image_name\n",
    "        return image, label\n",
    "\n",
    "\n",
    "labels = load_labels(DATASET_PATH)\n",
    "train_labels = labels[:TRAIN_TEST_PIVOT]\n",
    "test_labels = labels[TRAIN_TEST_PIVOT:]\n",
    "\n",
    "train_dataset = DatasetGenerator(\n",
    "    image_dir=IMAGE_DIR,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    labels=train_labels,\n",
    "    augmentation=False,\n",
    "    verbose=True,\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_dataset = DatasetGenerator(\n",
    "    image_dir=IMAGE_DIR,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    labels=test_labels,\n",
    "    augmentation=False,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 29\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    PAD_TOKEN = \"<pad>\"\n",
    "    SOS_TOKEN = \"<sos>\"\n",
    "    EOS_TOKEN = \"<eos>\"\n",
    "\n",
    "    PAD_INDEX = 0\n",
    "    SOS_INDEX = 1\n",
    "    EOS_INDEX = 2\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        self.n_words = 0\n",
    "\n",
    "        self.word2index[self.PAD_TOKEN] = self.PAD_INDEX\n",
    "        self.word2index[self.SOS_TOKEN] = self.SOS_INDEX\n",
    "        self.word2index[self.EOS_TOKEN] = self.EOS_INDEX\n",
    "        self.index2word[self.PAD_INDEX] = self.PAD_TOKEN\n",
    "        self.index2word[self.SOS_INDEX] = self.SOS_TOKEN\n",
    "        self.index2word[self.EOS_INDEX] = self.EOS_TOKEN\n",
    "        self.n_words = 3\n",
    "\n",
    "    def add_word(self, word: str):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "\n",
    "    def add_sentence(self, sentence: str):\n",
    "        word_list = [c for c in sentence]\n",
    "        for word in word_list:\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_sentences(self, sentences: list[str]):\n",
    "        for sentence in sentences:\n",
    "            self.add_sentence(sentence)\n",
    "\n",
    "    def sentence_to_tensor(\n",
    "        self,\n",
    "        sentence: str,\n",
    "        add_sos=True,\n",
    "        add_eos=True,\n",
    "        add_pad=False,\n",
    "        max_length=None,\n",
    "    ):\n",
    "        word_list = [c for c in sentence]\n",
    "        if add_sos:\n",
    "            word_list = [self.SOS_TOKEN] + word_list\n",
    "        if add_eos:\n",
    "            word_list = word_list + [self.EOS_TOKEN]\n",
    "        if add_pad and max_length is not None:\n",
    "            word_list = word_list[:max_length]\n",
    "            word_list += [self.PAD_TOKEN] * (max_length - len(word_list))\n",
    "        indices = [self.word2index[word] for word in word_list]\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def sentences_to_tensor(\n",
    "        self,\n",
    "        sentences: list[str],\n",
    "        add_sos=True,\n",
    "        add_eos=True,\n",
    "        add_pad=False,\n",
    "        max_length=None,\n",
    "    ):\n",
    "        tensors = []\n",
    "        for sentence in sentences:\n",
    "            tensor = self.sentence_to_tensor(\n",
    "                sentence, add_sos, add_eos, add_pad, max_length\n",
    "            )\n",
    "            tensors.append(tensor)\n",
    "        return torch.stack(tensors)\n",
    "\n",
    "    def tensor_to_sentence(\n",
    "        self,\n",
    "        tensor: torch.Tensor,\n",
    "        remove_sos=True,\n",
    "        remove_eos=True,\n",
    "        remove_pad=True,\n",
    "    ):\n",
    "        assert isinstance(\n",
    "            tensor, torch.Tensor\n",
    "        ), \"Input must be a tensor, got {}\".format(type(tensor))\n",
    "        if tensor.dim() == 1:\n",
    "            tensor = tensor.unsqueeze(0)\n",
    "        assert tensor.dim() == 2, \"Input must be a 1D or 2D tensor\"\n",
    "        sentences = []\n",
    "        for i in range(tensor.size(0)):\n",
    "            sentence = \"\"\n",
    "            for j in range(tensor.size(1)):\n",
    "                if remove_sos and tensor[i, j] == self.SOS_INDEX:\n",
    "                    continue\n",
    "                if remove_eos and tensor[i, j] == self.EOS_INDEX:\n",
    "                    continue\n",
    "                if remove_pad and tensor[i, j] == self.PAD_INDEX:\n",
    "                    continue\n",
    "                sentence += self.index2word[tensor[i, j].item()]\n",
    "            sentences.append(sentence)\n",
    "        return sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_words\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.add_sentences([label for _, label in train_labels])\n",
    "print(f\"Number of tokens: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "class DenseNet121Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super(DenseNet121Encoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        densenet = models.densenet121(\n",
    "            weights=models.DenseNet121_Weights.DEFAULT\n",
    "        )\n",
    "        self.cnn = densenet.features  # (batch_size, 1024, 8, 8)\n",
    "        self.fc = nn.Linear(\n",
    "            1024, embedding_dim\n",
    "        )  # (batch_size, 64, embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        :param x torch.Tensor: (batch_size, 3, 256, 256)\n",
    "        \"\"\"\n",
    "        x = self.cnn(x)  # (batch_size, 1024, 8, 8)\n",
    "        x = x.view(x.size(0), 1024, 64)  # (batch_size, 1024, 64)\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, 64, 1024)\n",
    "        x = self.fc(x)  # (batch_size, 64, embedding_dim)\n",
    "        x = self.relu(x)  # (batch_size, 64, embedding_dim)\n",
    "        return x\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_encoder(device):\n",
    "    encoder = DenseNet121Encoder(embedding_dim=EMBEDDING_DIM).to(device)\n",
    "    sample_input = torch.randn(BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE).to(\n",
    "        device\n",
    "    )\n",
    "    sample_output = encoder(sample_input)\n",
    "    print(sample_output.shape)\n",
    "\n",
    "\n",
    "test_encoder(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256]) torch.Size([16, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.W1 = nn.Linear(embedding_dim, hidden_dim)  # For features\n",
    "        self.W2 = nn.Linear(hidden_dim, hidden_dim)  # For hidden state\n",
    "        self.V = nn.Linear(hidden_dim, 1)  # For attention score\n",
    "\n",
    "    def forward(self, features: torch.Tensor, hidden: torch.Tensor):\n",
    "        \"\"\"\n",
    "        :param features torch.Tensor: (batch_size, seq_len, embedding_dim)\n",
    "        :param hidden torch.Tensor: (batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        hidden_with_time_axis = hidden.unsqueeze(\n",
    "            1\n",
    "        )  # (batch_size, 1, hidden_dim)\n",
    "        score = self.V(\n",
    "            torch.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        )  # (batch_size, seq_len, 1)\n",
    "        attention_weights = torch.softmax(\n",
    "            score, dim=1\n",
    "        )  # (batch_size, seq_len, 1)\n",
    "        context_vector = (\n",
    "            attention_weights * features\n",
    "        )  # (batch_size, seq_len, embedding_dim)\n",
    "        context_vector = torch.sum(\n",
    "            context_vector, dim=1\n",
    "        )  # (batch_size, embedding_dim)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_attention(device):\n",
    "    attention = Attention(\n",
    "        embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM\n",
    "    ).to(device)\n",
    "    sample_features = torch.randn(16, 10, EMBEDDING_DIM).to(device)\n",
    "    sample_hidden = torch.randn(16, HIDDEN_DIM).to(device)\n",
    "    sample_context, sample_attention_weight = attention(\n",
    "        sample_features, sample_hidden\n",
    "    )\n",
    "    print(sample_context.shape, sample_attention_weight.shape)\n",
    "\n",
    "\n",
    "test_attention(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 29]) torch.Size([16, 512]) torch.Size([16, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        hidden_dim: int,\n",
    "        vocab_size: int,\n",
    "    ):\n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.attention = Attention(\n",
    "            embedding_dim=embedding_dim, hidden_dim=hidden_dim\n",
    "        )  # (batch_size, hidden_dim), (batch_size, 1)\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embedding_dim\n",
    "        )  # (batch_size, 1, embedding_dim)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim + embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "        )  # (batch_size, 1, hidden_dim)\n",
    "        self.fc1 = nn.Linear(\n",
    "            hidden_dim, hidden_dim\n",
    "        )  # (batch_size, 1, hidden_dim)\n",
    "        self.fc2 = nn.Linear(\n",
    "            hidden_dim, vocab_size\n",
    "        )  # (batch_size, 1, vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        features: torch.Tensor,\n",
    "        hidden: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param x torch.Tensor: (batch_size, 1)\n",
    "        :param features torch.Tensor: (batch_size, 1, embedding_dim)\n",
    "        :param hidden torch.Tensor: (batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        context, attention_weight = self.attention(\n",
    "            features, hidden\n",
    "        )  # (batch_size, embedding_dim), (batch_size, 1)\n",
    "        x = self.embedding(x)  # (batch_size, 1, embedding_dim)\n",
    "        context = context.unsqueeze(1)  # (batch_size, 1, embedding_dim)\n",
    "        x = torch.cat(\n",
    "            [context, x], dim=2\n",
    "        )  # (batch_size, 1, embedding_dim + embedding_dim)\n",
    "        x, hidden = self.gru(\n",
    "            x, hidden.unsqueeze(0)\n",
    "        )  # (batch_size, 1, hidden_dim), (1, batch_size, hidden_dim)\n",
    "        x = self.fc1(x)  # (batch_size, 1, hidden_dim)\n",
    "        x = x.view(-1, x.size(2))  # (batch_size * 1, hidden_dim)\n",
    "        x = torch.relu(x)  # (batch_size * 1, hidden_dim)\n",
    "        x = self.fc2(x)  # (batch_size * 1, vocab_size)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        return x, hidden, attention_weight\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_decoder(device):\n",
    "    decoder = RNNDecoder(\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        vocab_size=len(tokenizer),\n",
    "    ).to(device)\n",
    "    sample_input = torch.randint(0, len(tokenizer), (16, 1)).to(device)\n",
    "    sample_features = torch.randn(16, 1, EMBEDDING_DIM).to(device)\n",
    "    sample_hidden = torch.randn(16, HIDDEN_DIM).to(device)\n",
    "    sample_output, sample_hidden, sample_attention_weight = decoder(\n",
    "        sample_input, sample_features, sample_hidden\n",
    "    )\n",
    "    print(\n",
    "        sample_output.shape,\n",
    "        sample_hidden.shape,\n",
    "        sample_attention_weight.shape,\n",
    "    )\n",
    "\n",
    "\n",
    "test_decoder(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10, 29])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class AttnBased(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        hidden_dim: int,\n",
    "        vocab_size: int,\n",
    "    ):\n",
    "        super(AttnBased, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.encoder = DenseNet121Encoder(embedding_dim=embedding_dim)\n",
    "        self.decoder = RNNDecoder(\n",
    "            embedding_dim=embedding_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            vocab_size=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        images: torch.Tensor,\n",
    "        targets: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param images torch.Tensor: (batch_size, 3, 256, 256)\n",
    "        :param targets torch.Tensor: (batch_size, seq_len)\n",
    "        :rtype: torch.Tensor\n",
    "        :return: (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        device = images.device\n",
    "        batch_size = images.size(0)\n",
    "        seq_len = targets.size(1)\n",
    "\n",
    "        hidden = torch.zeros(batch_size, self.hidden_dim).to(device)\n",
    "        outputs = torch.zeros(batch_size, seq_len, self.vocab_size).to(device)\n",
    "        outputs[:, 0, 1] = 1  # Set SOS token\n",
    "\n",
    "        features = self.encoder(images)  # (batch_size, 64, embedding_dim)\n",
    "        for t in range(seq_len - 1):\n",
    "            output, hidden, _ = self.decoder(\n",
    "                targets[:, t].unsqueeze(1), features, hidden\n",
    "            )\n",
    "            outputs[:, t + 1] = output\n",
    "        return outputs\n",
    "\n",
    "    def inference(self, images: torch.Tensor, max_length: int = 10):\n",
    "        \"\"\"\n",
    "        :param images torch.Tensor: (batch_size, 3, 256, 256)\n",
    "        :param max_length int: Maximum length of the output sequence\n",
    "        :rtype: torch.Tensor\n",
    "        :return: (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        device = images.device\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        hidden = torch.zeros(batch_size, self.hidden_dim).to(device)\n",
    "        outputs = torch.zeros(batch_size, max_length, self.vocab_size).to(\n",
    "            device\n",
    "        )\n",
    "        outputs[:, 0, 1] = 1\n",
    "\n",
    "        features = self.encoder(images)  # (batch_size, 64, embedding_dim)\n",
    "        for t in range(max_length - 1):\n",
    "            output, hidden, _ = self.decoder(\n",
    "                outputs[:, t].argmax(1).unsqueeze(1), features, hidden\n",
    "            )\n",
    "            outputs[:, t + 1] = output\n",
    "        return outputs\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_model(device):\n",
    "    model = AttnBased(\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        vocab_size=len(tokenizer),\n",
    "    ).to(device)\n",
    "    sample_images = torch.randn(16, 3, IMAGE_SIZE, IMAGE_SIZE).to(device)\n",
    "    sample_targets = torch.randint(0, len(tokenizer), (16, MAX_SEQ_LEN)).to(\n",
    "        device\n",
    "    )\n",
    "    sample_outputs = model(sample_images, sample_targets)\n",
    "    print(sample_outputs.shape)\n",
    "\n",
    "\n",
    "test_model(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 9471902\n",
      "Total parameters: 9471902\n"
     ]
    }
   ],
   "source": [
    "model = AttnBased(\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    vocab_size=len(tokenizer),\n",
    ").to(device)\n",
    "\n",
    "trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad\n",
    ")\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(f\"Total parameters: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4249)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.functional import F\n",
    "\n",
    "\n",
    "def loss_function(\n",
    "    predict: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "):\n",
    "    \"\"\"\n",
    "    :param predict torch.Tensor: (batch_size, seq_len, vocab_size)\n",
    "    :param target torch.Tensor: (batch_size, seq_len)\n",
    "    \"\"\"\n",
    "    mask = target != 0\n",
    "    predict = predict.view(\n",
    "        -1, predict.size(2)\n",
    "    )  # (batch_size * seq_len, vocab_size)\n",
    "    target = target.view(-1)  # (batch_size * seq_len)\n",
    "    mask = mask.view(-1)  # (batch_size * seq_len)\n",
    "    loss = F.cross_entropy(predict, target, reduction=\"none\")\n",
    "    loss = loss * mask\n",
    "    return loss.sum() / mask.sum()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_loss_function():\n",
    "    sample_predict = tokenizer.sentences_to_tensor(\n",
    "        [\"helo\", \"wo\"], add_pad=True, max_length=MAX_SEQ_LEN\n",
    "    )  # (2, 10)\n",
    "    sample_predict = F.one_hot(\n",
    "        sample_predict, num_classes=len(tokenizer)\n",
    "    ).float()  # (2, 10, 29)\n",
    "    sample_target = tokenizer.sentences_to_tensor(\n",
    "        [\"helo\", \"wo\"], add_pad=True, max_length=MAX_SEQ_LEN\n",
    "    )  # (2, 10)\n",
    "    sample_loss = loss_function(sample_predict, sample_target)\n",
    "    print(sample_loss)\n",
    "\n",
    "\n",
    "test_loss_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, checkpoint_dir, checkpoint_name):\n",
    "    \"\"\"\n",
    "    Save a checkpoint to a specified directory.\n",
    "\n",
    "    :param epoch: The epoch number to save in the checkpoint.\n",
    "    :param model: The model to save in the checkpoint.\n",
    "    :param optimizer: The optimizer to save in the checkpoint.\n",
    "    :param checkpoint_dir: The directory to save the checkpoint in.\n",
    "    :param checkpoint_name: The name of the checkpoint file, will be appended with the epoch number.\n",
    "    \"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(\n",
    "        checkpoint_dir, f\"{checkpoint_name}_{epoch:03d}.pt\"\n",
    "    )\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        },\n",
    "        checkpoint_path,\n",
    "    )\n",
    "    print(f\"Saved checkpoint for epoch {epoch} at {checkpoint_path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "    model,\n",
    "    checkpoint_dir,\n",
    "    checkpoint_name,\n",
    "    optimizer=None,\n",
    "    epoch=None,\n",
    "    device=None,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Load a checkpoint from a specified directory.\n",
    "\n",
    "    :param model: The model to load the checkpoint into.\n",
    "    :param checkpoint_dir: The directory to search for the checkpoint.\n",
    "    :param checkpoint_name: The name of the checkpoint file, will be appended with the epoch number.\n",
    "    :param optimizer: The optimizer to load the checkpoint into. If None, the optimizer is not loaded.\n",
    "    :param epoch: The epoch to load the checkpoint from. If None, the latest checkpoint is loaded.\n",
    "    If no checkpoint is found, the function prints a message and returns.\n",
    "    \"\"\"\n",
    "    if epoch is None:\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            print(\"No checkpoint found.\")\n",
    "            return False\n",
    "        # Search for the latest checkpoint\n",
    "        files = os.listdir(checkpoint_dir)\n",
    "        files = [\n",
    "            file\n",
    "            for file in files\n",
    "            if file.startswith(checkpoint_name) and file.endswith(\".pt\")\n",
    "        ]\n",
    "        if not files:\n",
    "            print(\"No checkpoint found.\")\n",
    "            return False\n",
    "        files.sort()\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, files[-1])\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(\n",
    "            checkpoint_dir, f\"{checkpoint_name}_{epoch:03d}.pt\"\n",
    "        )\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"Checkpoint for epoch {epoch} not found.\")\n",
    "        return False\n",
    "    checkpoint = torch.load(\n",
    "        checkpoint_path, weights_only=False, map_location=device\n",
    "    )\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "# Training step function\n",
    "def train_step(\n",
    "    model: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    loss_funcion: nn.Module,\n",
    "    images: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "):\n",
    "    model.train()  # Set model to training mode\n",
    "    optimizer.zero_grad()  # Zero out gradients\n",
    "    outputs = model(images, targets)  # Forward pass\n",
    "    loss = loss_funcion(outputs, targets)  # Compute loss\n",
    "    loss_metric = loss.item()\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update weights\n",
    "    return loss_metric\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "):\n",
    "    match_count = 0\n",
    "    total_count = 0\n",
    "    for idx, (images, labels) in enumerate(test_loader):\n",
    "        images = images.to(device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model.inference(images, max_length=MAX_SEQ_LEN)\n",
    "            sentence = tokenizer.tensor_to_sentence(outputs.argmax(2))\n",
    "        for pred, label in zip(sentence, labels):\n",
    "            total_count += 1\n",
    "            if pred == label:\n",
    "                match_count += 1\n",
    "    return match_count / total_count\n",
    "\n",
    "\n",
    "optimizer = None\n",
    "\n",
    "if EPOCHS > 0:\n",
    "    # Initialize\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    if START_EPOCH > 0:\n",
    "        # Load checkpoint\n",
    "        os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "        load_checkpoint(\n",
    "            model,\n",
    "            CHECKPOINT_DIR,\n",
    "            CHECKPOINT_NAME,\n",
    "            optimizer,\n",
    "            START_EPOCH,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "train_loss_list = []\n",
    "for epoch in range(START_EPOCH + 1, START_EPOCH + EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0\n",
    "\n",
    "    for idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        targets = tokenizer.sentences_to_tensor(\n",
    "            labels,\n",
    "            add_sos=True,\n",
    "            add_eos=True,\n",
    "            add_pad=True,\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "        ).to(device)\n",
    "        loss = train_step(model, optimizer, loss_function, images, targets)\n",
    "        train_loss += loss\n",
    "\n",
    "        if math.isnan(loss) or math.isinf(loss):\n",
    "            print(\"Loss is {}, stopping training\".format(loss))\n",
    "\n",
    "        if idx % 50 == 0:\n",
    "            print(\n",
    "                \"epoch {:03d}/{:03d}, batch {:03d}/{:03d}, loss {:.4f}\".format(\n",
    "                    epoch,\n",
    "                    START_EPOCH + EPOCHS,\n",
    "                    idx + 1,\n",
    "                    len(train_loader),\n",
    "                    loss,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    if math.isnan(loss) or math.isinf(loss):\n",
    "        print(f\"Loss is {loss}, stopping training\")\n",
    "\n",
    "    precision = evaluate(model)\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    print(\n",
    "        \"epoch {:03d}/{:03d}, loss {:.4f}, precision {:.4f}, time {:.2f}s\".format(\n",
    "            epoch,\n",
    "            START_EPOCH + EPOCHS,\n",
    "            avg_loss,\n",
    "            precision,\n",
    "            time.time() - start_time,\n",
    "        )\n",
    "    )\n",
    "    save_checkpoint(epoch, model, optimizer, CHECKPOINT_DIR, CHECKPOINT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record\n",
    "\n",
    "```\n",
    "epoch 001/020, loss 0.6165, precision 0.9760, time 671.98s\n",
    "epoch 002/020, loss 0.4132, precision 0.9884, time 668.52s\n",
    "epoch 003/020, loss 0.4099, precision 0.9909, time 667.80s\n",
    "epoch 004/020, loss 0.4090, precision 0.9935, time 663.10s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from ./ckpts/attn-v5/ckpt_004.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch = None\n",
    "load_checkpoint(\n",
    "    model, CHECKPOINT_DIR, CHECKPOINT_NAME, epoch=epoch, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.99345\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    test_dataset = DatasetGenerator(\n",
    "        image_dir=IMAGE_DIR,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        labels=test_labels,\n",
    "        augmentation=False,\n",
    "        return_name=True,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"output.txt\")\n",
    "\n",
    "    match_count = 0\n",
    "    total_count = 0\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for idx, (images, labels, names) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model.inference(images, max_length=MAX_SEQ_LEN)\n",
    "                sentence = tokenizer.tensor_to_sentence(outputs.argmax(2))\n",
    "            for pred, label, name in zip(sentence, labels, names):\n",
    "                total_count += 1\n",
    "                if pred == label:\n",
    "                    match_count += 1\n",
    "                f.write(f\"{name} {pred}\\n\")\n",
    "    return match_count / total_count\n",
    "\n",
    "\n",
    "ipy = get_ipython()\n",
    "if ipy is not None:\n",
    "    score = evaluate()\n",
    "    print(f\"Precision: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "First, I have tried to extract the features with densenet and train only on the features.\n",
    "However, the accuracy is only less than 1%.\n",
    "\n",
    "Then, I have tried to also train the cnn model, and the accuracy is increased to 99.35%.\n",
    "\n",
    "I have ported the code from tensorflow to pytorch as a practice.\n",
    "\n",
    "I found that the shape written in Lab12-2_slide is wrong. The output shape of the context vector in attention layer should be (batch_size, embedding_dim) instead of (batch_size, hidden_dim).\n",
    "\n",
    "I trained the model only for 4 epochs because it is enough to get the accuracy of 99.35%.\n",
    "And I train in the extracted python script so there is no training output in this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
