{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLab Cup 2: CNN for Object Detection\n",
    "\n",
    "Sao-Hsuan Lin\n",
    "\n",
    "113062532"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ID\n",
    "# TODO: change ID to the model name or experiment name\n",
    "ID = \"yolov8-b3-v2\"\n",
    "# ID = \"yolov8-b3-v3\"\n",
    "# ID = \"yolov8-b3-x-v1\"\n",
    "# ID = \"yolov8-dn121-v1\"\n",
    "\n",
    "# common params\n",
    "# TODO: change DEVICE\n",
    "DEVICE = \"cuda:2\"  # \"cuda:i\" or \"cpu\"\n",
    "OMP_NUM_THREADS = 10\n",
    "SEED = 42\n",
    "\n",
    "# dataset params\n",
    "# TODO: change the path to training data\n",
    "# TRAIN_DATA_PATH = \"./dataset/pascal_voc_training_data.txt\"\n",
    "# TRAIN_IMAGE_DIR = \"./dataset/VOCdevkit_train/VOC2007/JPEGImages/\"\n",
    "TRAIN_DATA_PATH = \"./dataset/augmented_data.txt\"\n",
    "TRAIN_IMAGE_DIR = \"./dataset/AugmentedImage/\"\n",
    "# TRAIN_DATA_PATH = \"./dataset/augmented_data_yolov8.txt\"\n",
    "# TRAIN_IMAGE_DIR = \"./dataset/AugmentedImageYoloV8/\"\n",
    "TEST_DATA_PATH = \"./dataset/pascal_voc_testing_data.txt\"\n",
    "TEST_IMAGE_DIR = \"./dataset/VOCdevkit_test/VOC2007/JPEGImages/\"\n",
    "\n",
    "# model params I\n",
    "IMAGE_SIZE = 640\n",
    "BATCH_SIZE = 16\n",
    "NUM_CLASSES = 20\n",
    "REG_MAX = 16\n",
    "MAX_OBJECTS_PER_IMAGE = 100\n",
    "\n",
    "# model params II\n",
    "# TODO: change the model size and weights\n",
    "MODEL_SIZE = \"m\"\n",
    "BOX_LOSS_WEIGHT = 7.5\n",
    "CLS_LOSS_WEIGHT = 0.5\n",
    "DFL_LOSS_WEIGHT = 1.5\n",
    "\n",
    "# training params\n",
    "# TODO: change the number of epochs\n",
    "START_EPOCH = 0\n",
    "EPOCHS = 0\n",
    "# TODO: change lr\n",
    "LEARNING_RATE = 1e-3\n",
    "# TODO: change if freeze backbone or training all\n",
    "FREEZE_BACKBONE = True\n",
    "\n",
    "# checkpoint params\n",
    "CHECKPOINT_DIR = os.path.join(\"./ckpts/\", ID)\n",
    "CHECKPOINT_NAME = \"yolov8_checkpoint\"\n",
    "\n",
    "# evaluation params\n",
    "OUTPUT_DIR = os.path.join(\"./output/\", ID)\n",
    "PRED_OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"yolo_predictions.csv\")\n",
    "EVAL_OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"yolo_eval_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 4\n",
      "Device: cuda:2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fad4680c610>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {gpus}\")\n",
    "    device = torch.device(DEVICE)\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(OMP_NUM_THREADS)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 25406076\n",
      "Total parameters: 35509428\n",
      "Ratio: 0.715\n"
     ]
    }
   ],
   "source": [
    "from models.yolov8.layers import (\n",
    "    YoloV8WithResNet,\n",
    "    YoloV8WithEfficientNetB3,\n",
    "    YoloV8WithDenseNet121,\n",
    ")\n",
    "\n",
    "model_cls = YoloV8WithEfficientNetB3\n",
    "if MODEL_SIZE == \"n\":\n",
    "    model = model_cls.get_yolo_v8_n(\n",
    "        num_classes=NUM_CLASSES,\n",
    "        reg_max=REG_MAX,\n",
    "        pred_max=MAX_OBJECTS_PER_IMAGE,\n",
    "    )\n",
    "elif MODEL_SIZE == \"m\":\n",
    "    model = model_cls.get_yolo_v8_m(\n",
    "        num_classes=NUM_CLASSES,\n",
    "        reg_max=REG_MAX,\n",
    "        pred_max=MAX_OBJECTS_PER_IMAGE,\n",
    "    )\n",
    "elif MODEL_SIZE == \"x\":\n",
    "    model = model_cls.get_yolo_v8_x(\n",
    "        num_classes=NUM_CLASSES,\n",
    "        reg_max=REG_MAX,\n",
    "        pred_max=MAX_OBJECTS_PER_IMAGE,\n",
    "    )\n",
    "model = model.to(device)\n",
    "\n",
    "trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad\n",
    ")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\n",
    "    \"Trainable parameters: {}\\nTotal parameters: {}\\nRatio: {:5.3f}\".format(\n",
    "        trainable_params,\n",
    "        total_params,\n",
    "        trainable_params / total_params,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from models.yolov8.data import (\n",
    "    TrainDatasetGenerator,\n",
    "    train_collate_fn,\n",
    ")\n",
    "\n",
    "\n",
    "def create_data_loader(\n",
    "    data_path,\n",
    "    image_dir,\n",
    "    batch_size,\n",
    "    image_size,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    device: str = \"\",\n",
    "):\n",
    "    dataset = TrainDatasetGenerator(\n",
    "        data_path,\n",
    "        image_dir,\n",
    "        image_size,\n",
    "    )\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        pin_memory_device=device,\n",
    "        collate_fn=train_collate_fn,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        pin_memory_device=device,\n",
    "        collate_fn=train_collate_fn,\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 02:52:52.579298, start training.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from datetime import datetime\n",
    "from models.yolov8.layers import YoloV8, YoloV8Loss\n",
    "from utils.training import load_checkpoint, save_checkpoint\n",
    "\n",
    "\n",
    "# Training step function\n",
    "def train_step(\n",
    "    model: YoloV8,\n",
    "    optimizer: optim.Optimizer,\n",
    "    images: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "):\n",
    "    model.train()  # Set model to training mode\n",
    "    optimizer.zero_grad()  # Zero out gradients\n",
    "    outputs = model(images)  # Forward pass\n",
    "    loss = yolo_loss(outputs, labels)  # Compute loss\n",
    "    loss_metric = loss.item()\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update weights\n",
    "    return loss_metric\n",
    "\n",
    "\n",
    "def val_step(model: YoloV8, images: torch.Tensor, labels: torch.Tensor):\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        outputs = model(images)\n",
    "        loss = yolo_loss(outputs, labels)\n",
    "        loss_metric = loss.item()\n",
    "    return loss_metric\n",
    "\n",
    "\n",
    "# Directory for saving checkpoints\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "train_loader, val_loader = create_data_loader(\n",
    "    TRAIN_DATA_PATH,\n",
    "    TRAIN_IMAGE_DIR,\n",
    "    BATCH_SIZE,\n",
    "    IMAGE_SIZE,\n",
    ")\n",
    "\n",
    "yolo_loss = None\n",
    "optimizer = None\n",
    "scheduler = None\n",
    "\n",
    "if EPOCHS > 0:\n",
    "    # Initialize objects if need training\n",
    "    yolo_loss = YoloV8Loss(\n",
    "        model,\n",
    "        box_loss_weight=BOX_LOSS_WEIGHT,\n",
    "        cls_loss_weight=CLS_LOSS_WEIGHT,\n",
    "        dfl_loss_weight=DFL_LOSS_WEIGHT,\n",
    "    ).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "    # Load checkpoint if available\n",
    "    if START_EPOCH > 0:\n",
    "        load_checkpoint(\n",
    "            model, CHECKPOINT_DIR, CHECKPOINT_NAME, optimizer, START_EPOCH\n",
    "        )\n",
    "\n",
    "    # Force set lr\n",
    "    optimizer.param_groups[0][\"lr\"] = LEARNING_RATE\n",
    "\n",
    "    # Freeze/Unfreeze backbone\n",
    "    if FREEZE_BACKBONE:\n",
    "        model.freeze_backbone()\n",
    "    else:\n",
    "        model.unfreeze_backbone()\n",
    "\n",
    "\n",
    "error_breaking = False\n",
    "\n",
    "# Training loop\n",
    "print(f\"{datetime.now()}, start training.\")\n",
    "for epoch in range(START_EPOCH, EPOCHS + START_EPOCH):\n",
    "    start_ts = time.perf_counter()\n",
    "    loss_metric_list = []\n",
    "    loss_detail_list = []\n",
    "    val_loss_metric_list = []\n",
    "\n",
    "    for idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = (\n",
    "            images.to(device),\n",
    "            labels.to(device),\n",
    "        )\n",
    "        loss_metric = train_step(model, optimizer, images, labels)\n",
    "        loss_metric_list.append(loss_metric)\n",
    "        loss_detail_list.append(yolo_loss._prev_loss.cpu().detach().numpy())\n",
    "\n",
    "        if (\n",
    "            math.isnan(loss_metric)\n",
    "            or math.isinf(loss_metric)\n",
    "            or loss_metric < 0\n",
    "            or (yolo_loss._prev_loss < 0).any()\n",
    "        ):\n",
    "            print(\"Loss is {:.4f}, stop training.\".format(loss_metric))\n",
    "            error_breaking = True\n",
    "            break\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            lr = scheduler.get_lr()[0]\n",
    "            print(\n",
    "                \"epoch {:3d}/{:3d}, batch: {:4d}/{:4d}, loss {:10.4f} [{:3f}, {:3f}, {:3f}], lr {:10.4e}\".format(\n",
    "                    epoch + 1,\n",
    "                    EPOCHS + START_EPOCH,\n",
    "                    idx + 1,\n",
    "                    len(train_loader),\n",
    "                    loss_metric,\n",
    "                    yolo_loss._prev_loss[0],\n",
    "                    yolo_loss._prev_loss[1],\n",
    "                    yolo_loss._prev_loss[2],\n",
    "                    lr,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    if error_breaking:\n",
    "        break\n",
    "\n",
    "    # Scheduler step after each epoch\n",
    "    # scheduler.step()\n",
    "\n",
    "    for idx, (images, labels) in enumerate(val_loader):\n",
    "        images, labels = (\n",
    "            images.to(device),\n",
    "            labels.to(device),\n",
    "        )\n",
    "        val_loss_metric = val_step(model, images, labels)\n",
    "        val_loss_metric_list.append(val_loss_metric)\n",
    "\n",
    "    # Print info\n",
    "    avg_train_loss = sum(loss_metric_list) / len(loss_metric_list)\n",
    "    ave_train_loss_detail = np.mean(loss_detail_list, axis=0)\n",
    "    avg_val_loss = sum(val_loss_metric_list) / len(val_loss_metric_list)\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    print(\n",
    "        \"epoch {:3d}/{:3d}, train loss {:10.4f}, {}, val loss {:10.4f}, lr {:10.4e}, time {:.2f}s\".format(\n",
    "            epoch + 1,\n",
    "            EPOCHS + START_EPOCH,\n",
    "            avg_train_loss,\n",
    "            ave_train_loss_detail,\n",
    "            avg_val_loss,\n",
    "            lr,\n",
    "            time.perf_counter() - start_ts,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(\n",
    "        epoch + 1, model, optimizer, CHECKPOINT_DIR, CHECKPOINT_NAME\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from ./ckpts/yolov8-b3-v2/yolov8_checkpoint_010.pt\n"
     ]
    }
   ],
   "source": [
    "# Load model from checkpoint\n",
    "load_checkpoint(model, CHECKPOINT_DIR, CHECKPOINT_NAME, epoch=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from IPython import get_ipython\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.data import CLASS_NAMES\n",
    "from models.yolov8.layers import YoloV8\n",
    "from models.yolov8.utils import process_outputs\n",
    "\n",
    "\n",
    "def predict_draw(image_path, model: YoloV8):\n",
    "    np_img = cv2.imread(image_path)\n",
    "    np_img = cv2.resize(np_img, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    np_img = cv2.cvtColor(np_img, cv2.COLOR_BGR2RGB)\n",
    "    resized_img = np_img\n",
    "    np_img = np_img.astype(np.float32)\n",
    "    np_img = np_img / 255.0 * 2 - 1\n",
    "    np_img = np.reshape(np_img, (1, IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    np_img = np.transpose(np_img, (0, 3, 1, 2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        y_pred = (\n",
    "            model.inference(torch.tensor(np_img).to(device)).cpu().detach()\n",
    "        )\n",
    "    bbox_list, class_list, conf_list = process_outputs(\n",
    "        y_pred, IMAGE_SIZE, conf_threshold=0.0, conf_ratio=0.01\n",
    "    )\n",
    "\n",
    "    bboxes, classes, confidences = (\n",
    "        bbox_list[0],\n",
    "        class_list[0],\n",
    "        conf_list[0],\n",
    "    )\n",
    "    for idx, (bbox, class_idx, conf) in enumerate(\n",
    "        zip(bboxes, classes, confidences)\n",
    "    ):\n",
    "        if idx >= 4:\n",
    "            break\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        cv2.rectangle(\n",
    "            resized_img,\n",
    "            (int(xmin), int(ymin)),\n",
    "            (int(xmax), int(ymax)),\n",
    "            (0, 255, 255),\n",
    "            2,\n",
    "        )\n",
    "        txt = f\"{CLASS_NAMES[class_idx]}: {conf:.2f}\"\n",
    "        cv2.putText(\n",
    "            resized_img,\n",
    "            txt,\n",
    "            (int(xmin) + 8, int(ymin) + 18),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7,\n",
    "            (0, 0, 0),\n",
    "            3,\n",
    "            cv2.LINE_8,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            resized_img,\n",
    "            txt,\n",
    "            (int(xmin) + 8, int(ymin) + 18),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7,\n",
    "            (0, 220, 0),\n",
    "            2,\n",
    "            cv2.LINE_8,\n",
    "        )\n",
    "    return resized_img\n",
    "\n",
    "\n",
    "def visualize(data_path, image_dir, model, shuffle=False):\n",
    "    # Retrieve image names\n",
    "    image_names = []\n",
    "    with open(data_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            image_names.append(line.strip().split(\" \")[0])\n",
    "    image_names = (\n",
    "        random.sample(image_names, 25) if shuffle else image_names[:25]\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for image_name in tqdm(image_names):\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "        image = predict_draw(image_path, model)\n",
    "        results.append((image, image_name))\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i, (img, title) in enumerate(results):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "ipy = get_ipython()\n",
    "if ipy is not None:\n",
    "    ipy.run_line_magic(\"matplotlib\", \"inline\")\n",
    "\n",
    "    random.seed(SEED)\n",
    "    visualize(TRAIN_DATA_PATH, TRAIN_IMAGE_DIR, model, shuffle=True)\n",
    "    visualize(TEST_DATA_PATH, TEST_IMAGE_DIR, model)\n",
    "\n",
    "    with open(TEST_DATA_PATH, \"r\") as f:\n",
    "        test_data = f.readlines()\n",
    "\n",
    "    # image_name = random.choice(test_data).strip().split(\" \")[0]\n",
    "    # image_path = os.path.join(TEST_IMAGE_DIR, image_name)\n",
    "    # image_path = \"./dataset/VOCdevkit_test/VOC2007/JPEGImages/000002.jpg\"\n",
    "    # print(f\"Read image: {image_path}\")\n",
    "    # image = predict_draw(image_path, model)\n",
    "    # plt.imshow(image)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence ratio: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/307 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [01:51<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Evalutation\n",
      "Predict saved to ./output/yolov8-b3-v2/yolo_predictions_0.1.csv\n",
      "Evaluation saved to ./output/yolov8-b3-v2/yolo_eval_results.csv\n",
      "Score 0.4199\n",
      "\n",
      "Confidence ratio: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [01:29<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Evalutation\n",
      "Predict saved to ./output/yolov8-b3-v2/yolo_predictions_0.2.csv\n",
      "Evaluation saved to ./output/yolov8-b3-v2/yolo_eval_results.csv\n",
      "Score 0.4199\n",
      "\n",
      "Confidence ratio: 0.30000000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [01:20<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Evalutation\n",
      "Predict saved to ./output/yolov8-b3-v2/yolo_predictions_0.3.csv\n",
      "Evaluation saved to ./output/yolov8-b3-v2/yolo_eval_results.csv\n",
      "Score 0.4240\n",
      "\n",
      "Confidence ratio: 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [01:14<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Evalutation\n",
      "Predict saved to ./output/yolov8-b3-v2/yolo_predictions_0.4.csv\n",
      "Evaluation saved to ./output/yolov8-b3-v2/yolo_eval_results.csv\n",
      "Score 0.4327\n",
      "\n",
      "Confidence ratio: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [01:11<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Evalutation\n",
      "Predict saved to ./output/yolov8-b3-v2/yolo_predictions_0.5.csv\n",
      "Evaluation saved to ./output/yolov8-b3-v2/yolo_eval_results.csv\n",
      "Score 0.4415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from models.yolov8.layers import YoloV8\n",
    "from models.yolov8.data import TestDatasetGenerator\n",
    "from utils.evaluate import evaluate\n",
    "from compute_score import compute_score\n",
    "\n",
    "\n",
    "def predict(\n",
    "    model: YoloV8,\n",
    "    test_data_path: str,\n",
    "    test_image_dir: str,\n",
    "    image_size: int,\n",
    "    batch_size: int,\n",
    "    pred_output_path: str,\n",
    "    conf_threshold: float = 0.0,\n",
    "    conf_ratio: float = 0.4,\n",
    "):\n",
    "    # Test data loader\n",
    "    data_loader = DataLoader(\n",
    "        TestDatasetGenerator(test_data_path, test_image_dir, image_size),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    # Test the model\n",
    "    # Output format: image_name {xmin_i ymin_i xmax_i ymax_i class_i confidence_score} (repeat number of objects times)\n",
    "    if not os.path.exists(os.path.dirname(pred_output_path)):\n",
    "        os.makedirs(os.path.dirname(pred_output_path))\n",
    "    with open(pred_output_path, \"w\") as output_file:\n",
    "        for image_names, images, image_heights, image_widths in tqdm(\n",
    "            data_loader\n",
    "        ):\n",
    "            images, image_heights, image_widths = (\n",
    "                images.to(device),\n",
    "                image_heights.to(device),\n",
    "                image_widths.to(device),\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                outputs = model.inference(images).cpu().detach()\n",
    "            bbox_list, class_list, conf_list = process_outputs(\n",
    "                outputs,\n",
    "                IMAGE_SIZE,\n",
    "                conf_threshold=conf_threshold,\n",
    "                conf_ratio=conf_ratio,\n",
    "            )\n",
    "            for batch_idx in range(images.size(0)):\n",
    "                answers = []\n",
    "                bboxes, classes, confidences = (\n",
    "                    bbox_list[batch_idx],\n",
    "                    class_list[batch_idx],\n",
    "                    conf_list[batch_idx],\n",
    "                )\n",
    "                for bbox, class_idx, conf in zip(bboxes, classes, confidences):\n",
    "                    xmin, ymin, xmax, ymax = bbox\n",
    "                    xmin, ymin, xmax, ymax = (\n",
    "                        xmin * (image_widths[batch_idx] / IMAGE_SIZE),\n",
    "                        ymin * (image_heights[batch_idx] / IMAGE_SIZE),\n",
    "                        xmax * (image_widths[batch_idx] / IMAGE_SIZE),\n",
    "                        ymax * (image_heights[batch_idx] / IMAGE_SIZE),\n",
    "                    )\n",
    "                    answers.append(\n",
    "                        \"%d %d %d %d %d %f\"\n",
    "                        % (xmin, ymin, xmax, ymax, class_idx, conf)\n",
    "                    )\n",
    "                output_file.write(\n",
    "                    image_names[batch_idx] + \" \" + \" \".join(answers) + \"\\n\"\n",
    "                )\n",
    "\n",
    "\n",
    "for conf_ratio in np.arange(0.1, 0.6, 0.1):\n",
    "    print(\"Confidence ratio:\", conf_ratio)\n",
    "    PRED_OUTPUT_PATH = os.path.join(\n",
    "        OUTPUT_DIR, f\"yolo_predictions_{conf_ratio:.1f}.csv\"\n",
    "    )\n",
    "    predict(\n",
    "        model,\n",
    "        TEST_DATA_PATH,\n",
    "        TEST_IMAGE_DIR,\n",
    "        IMAGE_SIZE,\n",
    "        BATCH_SIZE,\n",
    "        PRED_OUTPUT_PATH,\n",
    "        conf_ratio=conf_ratio,\n",
    "    )\n",
    "    evaluate(PRED_OUTPUT_PATH, EVAL_OUTPUT_PATH)\n",
    "    print(\"Predict saved to\", PRED_OUTPUT_PATH)\n",
    "    print(\"Evaluation saved to\", EVAL_OUTPUT_PATH)\n",
    "    print(\"Score {:.4f}\".format(compute_score(EVAL_OUTPUT_PATH)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from ./ckpts/yolov8-b3-v2/yolov8_checkpoint_001.pt\n",
      "Confidence ratio: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [02:25<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Evalutation\n",
      "Evaluation saved to ./output/yolov8-b3-v2/yolo_eval_results.csv\n",
      "Score 0.6919\n",
      "\n",
      "Loaded checkpoint from ./ckpts/yolov8-b3-v2/yolov8_checkpoint_002.pt\n",
      "Confidence ratio: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [02:13<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Evalutation\n",
      "Evaluation saved to ./output/yolov8-b3-v2/yolo_eval_results.csv\n",
      "Score 0.6080\n",
      "\n",
      "Loaded checkpoint from ./ckpts/yolov8-b3-v2/yolov8_checkpoint_003.pt\n",
      "Confidence ratio: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [01:58<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Evalutation\n",
      "Evaluation saved to ./output/yolov8-b3-v2/yolo_eval_results.csv\n",
      "Score 0.5686\n",
      "\n",
      "Loaded checkpoint from ./ckpts/yolov8-b3-v2/yolov8_checkpoint_004.pt\n",
      "Confidence ratio: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [01:54<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Evalutation\n",
      "Evaluation saved to ./output/yolov8-b3-v2/yolo_eval_results.csv\n",
      "Score 0.5024\n",
      "\n",
      "Loaded checkpoint from ./ckpts/yolov8-b3-v2/yolov8_checkpoint_005.pt\n",
      "Confidence ratio: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [01:52<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Evalutation\n",
      "Evaluation saved to ./output/yolov8-b3-v2/yolo_eval_results.csv\n",
      "Score 0.4618\n",
      "\n",
      "Loaded checkpoint from ./ckpts/yolov8-b3-v2/yolov8_checkpoint_006.pt\n",
      "Confidence ratio: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [01:46<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Evalutation\n",
      "Evaluation saved to ./output/yolov8-b3-v2/yolo_eval_results.csv\n",
      "Score 0.4502\n",
      "\n",
      "Loaded checkpoint from ./ckpts/yolov8-b3-v2/yolov8_checkpoint_007.pt\n",
      "Confidence ratio: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [01:51<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Evalutation\n",
      "Evaluation saved to ./output/yolov8-b3-v2/yolo_eval_results.csv\n",
      "Score 0.4546\n",
      "\n",
      "Loaded checkpoint from ./ckpts/yolov8-b3-v2/yolov8_checkpoint_008.pt\n",
      "Confidence ratio: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [02:00<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Evalutation\n",
      "Evaluation saved to ./output/yolov8-b3-v2/yolo_eval_results.csv\n",
      "Score 0.4714\n",
      "\n",
      "Loaded checkpoint from ./ckpts/yolov8-b3-v2/yolov8_checkpoint_009.pt\n",
      "Confidence ratio: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [01:51<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Evalutation\n",
      "Evaluation saved to ./output/yolov8-b3-v2/yolo_eval_results.csv\n",
      "Score 0.4133\n",
      "\n",
      "Loaded checkpoint from ./ckpts/yolov8-b3-v2/yolov8_checkpoint_010.pt\n",
      "Confidence ratio: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [01:49<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Evalutation\n",
      "Evaluation saved to ./output/yolov8-b3-v2/yolo_eval_results.csv\n",
      "Score 0.4199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    # Load model from checkpoint\n",
    "    load_checkpoint(model, CHECKPOINT_DIR, CHECKPOINT_NAME, epoch=epoch)\n",
    "\n",
    "    conf_ratio = 0.1\n",
    "    print(\"Confidence ratio:\", conf_ratio)\n",
    "    PRED_OUTPUT_PATH = os.path.join(\n",
    "        OUTPUT_DIR, f\"yolo_predictions_{conf_ratio:.1f}.csv\"\n",
    "    )\n",
    "    predict(\n",
    "        model,\n",
    "        TEST_DATA_PATH,\n",
    "        TEST_IMAGE_DIR,\n",
    "        IMAGE_SIZE,\n",
    "        BATCH_SIZE,\n",
    "        PRED_OUTPUT_PATH,\n",
    "        conf_ratio=conf_ratio,\n",
    "    )\n",
    "    evaluate(PRED_OUTPUT_PATH, EVAL_OUTPUT_PATH)\n",
    "    print(\"Evaluation saved to\", EVAL_OUTPUT_PATH)\n",
    "    print(\"Score {:.4f}\".format(compute_score(EVAL_OUTPUT_PATH)))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
