{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLab Cup 2: CNN for Object Detection\n",
    "\n",
    "Sao-Hsuan Lin\n",
    "\n",
    "113062532"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from uuid import uuid4\n",
    "from models.yolov8.layers import (\n",
    "    YoloV8,\n",
    "    YoloV8WithResNet,\n",
    "    YoloV8WithEfficientNetB3,\n",
    "    YoloV8WithDenseNet121,\n",
    ")\n",
    "\n",
    "# ID\n",
    "# TODO: change ID to the model name or experiment name\n",
    "ID = uuid4().hex[:8]\n",
    "# ID = \"yolov8-b3-m-v2\"\n",
    "# ID = \"yolov8-b3-m-aug-exp\"\n",
    "# ID = \"yolov8-b3-x-v1\"\n",
    "# ID = \"yolov8-b3-x-v2\"\n",
    "# ID = \"yolov8-b3-x-v3\"\n",
    "ID = \"yolov8-b3-x-v3-2\"\n",
    "\n",
    "# common params\n",
    "# TODO: change DEVICE\n",
    "DEVICE = \"cuda:0\"  # \"cuda:i\" or \"cpu\"\n",
    "OMP_NUM_THREADS = 10\n",
    "SEED = 42\n",
    "\n",
    "# dataset params\n",
    "# TODO: change the path to training data\n",
    "TRAIN_DATA_PATH = \"./dataset/pascal_voc_training_data.txt\"\n",
    "TRAIN_IMAGE_DIR = \"./dataset/VOCdevkit_train/VOC2007/JPEGImages/\"\n",
    "TEST_DATA_PATH = \"./dataset/pascal_voc_testing_data.txt\"\n",
    "TEST_IMAGE_DIR = \"./dataset/VOCdevkit_test/VOC2007/JPEGImages/\"\n",
    "\n",
    "# model params I\n",
    "IMAGE_SIZE = 640\n",
    "BATCH_SIZE = 16\n",
    "NUM_CLASSES = 20\n",
    "REG_MAX = 10\n",
    "MAX_OBJECTS_PER_IMAGE = 100\n",
    "\n",
    "# model params II\n",
    "# TODO: change the model class to the yolov8 model you want to train\n",
    "MODEL_CLS = YoloV8WithEfficientNetB3\n",
    "# TODO: change the model size, options: \"n\", \"s\", \"m\", \"l\", \"x\"\n",
    "MODEL_SIZE = \"x\"\n",
    "# TODO: change the model and weights\n",
    "BOX_LOSS_WEIGHT = 7.5\n",
    "CLS_LOSS_WEIGHT = 3.5\n",
    "DFL_LOSS_WEIGHT = 1.5\n",
    "\n",
    "# training params\n",
    "# TODO: change the number of epochs\n",
    "START_EPOCH = 0\n",
    "EPOCHS = 0\n",
    "# TODO: change lr\n",
    "LEARNING_RATE = 1e-3\n",
    "# TODO: change if freeze backbone or training all\n",
    "FREEZE_BACKBONE = True\n",
    "\n",
    "# checkpoint params\n",
    "CHECKPOINT_DIR = os.path.join(\"./ckpts/\", ID)\n",
    "CHECKPOINT_NAME = \"yolov8_checkpoint\"\n",
    "\n",
    "# evaluation params\n",
    "OUTPUT_DIR = os.path.join(\"./output/\", ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {gpus}\")\n",
    "    device = torch.device(DEVICE)\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(OMP_NUM_THREADS)\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_SIZE == \"n\":\n",
    "    model = MODEL_CLS.get_yolo_v8_n(\n",
    "        num_classes=NUM_CLASSES,\n",
    "        reg_max=REG_MAX,\n",
    "        pred_max=MAX_OBJECTS_PER_IMAGE,\n",
    "    )\n",
    "elif MODEL_SIZE == \"m\":\n",
    "    model = MODEL_CLS.get_yolo_v8_m(\n",
    "        num_classes=NUM_CLASSES,\n",
    "        reg_max=REG_MAX,\n",
    "        pred_max=MAX_OBJECTS_PER_IMAGE,\n",
    "    )\n",
    "elif MODEL_SIZE == \"x\":\n",
    "    model = MODEL_CLS.get_yolo_v8_x(\n",
    "        num_classes=NUM_CLASSES,\n",
    "        reg_max=REG_MAX,\n",
    "        pred_max=MAX_OBJECTS_PER_IMAGE,\n",
    "    )\n",
    "model = model.to(device)\n",
    "\n",
    "trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad\n",
    ")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\n",
    "    \"Trainable parameters: {}\\nTotal parameters: {}\\nRatio: {:5.3f}\".format(\n",
    "        trainable_params,\n",
    "        total_params,\n",
    "        trainable_params / total_params,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from models.yolov8.data import (\n",
    "    AugmentedTrainDatasetGeneratorV2,\n",
    "    train_collate_fn,\n",
    ")\n",
    "\n",
    "\n",
    "def create_data_loader(\n",
    "    data_path,\n",
    "    image_dir,\n",
    "    batch_size,\n",
    "    image_size,\n",
    "    shuffle=True,\n",
    "    num_workers=12,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    device: str = \"\",\n",
    "):\n",
    "    dataset = AugmentedTrainDatasetGeneratorV2(\n",
    "        data_path, image_dir, image_size, p_cutmix=1\n",
    "    )\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        pin_memory_device=device,\n",
    "        collate_fn=train_collate_fn,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        pin_memory_device=device,\n",
    "        collate_fn=train_collate_fn,\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "# Plot some images from the training loader\n",
    "from IPython import get_ipython\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from utils.data import CLASS_NAMES\n",
    "\n",
    "ipy = get_ipython()\n",
    "if ipy is not None:\n",
    "    ipy.run_line_magic(\"matplotlib\", \"inline\")\n",
    "    train_loader, val_loader = create_data_loader(\n",
    "        TRAIN_DATA_PATH,\n",
    "        TRAIN_IMAGE_DIR,\n",
    "        BATCH_SIZE,\n",
    "        IMAGE_SIZE,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    idx = 0\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    for images, targets in train_loader:\n",
    "        images = images.cpu().numpy()\n",
    "        labels = targets.cpu().numpy()\n",
    "\n",
    "        images = ((images + 1) / 2 * 255).astype(np.uint8)\n",
    "        images = images.transpose(0, 2, 3, 1)\n",
    "        labels[:, 1:5] = labels[:, 1:5] * IMAGE_SIZE\n",
    "        for batch_idx in range(images.shape[0]):\n",
    "            image = images[batch_idx].copy()\n",
    "            for x1, y1, x2, y2, cls in labels[labels[:, 0] == batch_idx][\n",
    "                :, 1:\n",
    "            ]:\n",
    "                image = cv2.rectangle(\n",
    "                    image,\n",
    "                    (int(x1), int(y1)),\n",
    "                    (int(x2), int(y2)),\n",
    "                    (0, 255, 255),\n",
    "                    5,\n",
    "                )\n",
    "                txt = \"{}\".format(CLASS_NAMES[int(cls)])\n",
    "                cv2.putText(\n",
    "                    image,\n",
    "                    txt,\n",
    "                    (int(x1) + 10, int(y1) + 25),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1,\n",
    "                    (0, 0, 0),\n",
    "                    6,\n",
    "                )\n",
    "                cv2.putText(\n",
    "                    image,\n",
    "                    txt,\n",
    "                    (int(x1) + 10, int(y1) + 25),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1,\n",
    "                    (0, 255, 255),\n",
    "                    4,\n",
    "                )\n",
    "            plt.subplot(2, 5, idx + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"Batch {batch_idx}\")\n",
    "            idx += 1\n",
    "            if idx >= 10:\n",
    "                break\n",
    "        if idx >= 10:\n",
    "            break\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from datetime import datetime\n",
    "from utils.training import load_checkpoint, save_checkpoint\n",
    "from models.yolov8.layers import YoloV8, YoloV8Loss\n",
    "from models.yolov8.evaluate import predict_and_evaluate\n",
    "\n",
    "\n",
    "# Training step function\n",
    "def train_step(\n",
    "    model: YoloV8,\n",
    "    optimizer: optim.Optimizer,\n",
    "    images: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "):\n",
    "    model.train()  # Set model to training mode\n",
    "    optimizer.zero_grad()  # Zero out gradients\n",
    "    outputs = model(images)  # Forward pass\n",
    "    loss = yolo_loss(outputs, labels)  # Compute loss\n",
    "    loss_metric = loss.item()\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update weights\n",
    "    return loss_metric\n",
    "\n",
    "\n",
    "def val_step(model: YoloV8, images: torch.Tensor, labels: torch.Tensor):\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        outputs = model(images)\n",
    "        loss = yolo_loss(outputs, labels)\n",
    "        loss_metric = loss.item()\n",
    "    return loss_metric\n",
    "\n",
    "\n",
    "# Directory for saving checkpoints\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "train_loader, val_loader = None, None\n",
    "yolo_loss = None\n",
    "optimizer = None\n",
    "scheduler = None\n",
    "error_breaking = False\n",
    "\n",
    "if EPOCHS > 0:\n",
    "    # Initialize objects if need training\n",
    "    train_loader, val_loader = create_data_loader(\n",
    "        TRAIN_DATA_PATH,\n",
    "        TRAIN_IMAGE_DIR,\n",
    "        BATCH_SIZE,\n",
    "        IMAGE_SIZE,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    yolo_loss = YoloV8Loss(\n",
    "        model,\n",
    "        box_loss_weight=BOX_LOSS_WEIGHT,\n",
    "        cls_loss_weight=CLS_LOSS_WEIGHT,\n",
    "        dfl_loss_weight=DFL_LOSS_WEIGHT,\n",
    "    ).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "    # Load checkpoint if available\n",
    "    if START_EPOCH > 0:\n",
    "        load_checkpoint(\n",
    "            model,\n",
    "            CHECKPOINT_DIR,\n",
    "            CHECKPOINT_NAME,\n",
    "            optimizer,\n",
    "            START_EPOCH,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    # TODO: Force set lr if needed\n",
    "    optimizer.param_groups[0][\"lr\"] = LEARNING_RATE\n",
    "\n",
    "    # Freeze/Unfreeze backbone\n",
    "    if FREEZE_BACKBONE:\n",
    "        model.freeze_backbone()\n",
    "    else:\n",
    "        model.unfreeze_backbone()\n",
    "\n",
    "    # Print parameters\n",
    "    print(\"=== Info ===\")\n",
    "    print(\"Training ID: {}\".format(ID))\n",
    "    print(\"Training device: {}\".format(str(device)))\n",
    "    print(\"=== Dataset ===\")\n",
    "    print(\"Dataset path: {}\".format(TRAIN_DATA_PATH))\n",
    "    print(\"Training on {} images\".format(len(train_loader.dataset)))\n",
    "    print(\"Validating on {} images\".format(len(val_loader.dataset)))\n",
    "    print(\"Image size: {}\".format(IMAGE_SIZE))\n",
    "    print(\"Batch size: {}\".format(BATCH_SIZE))\n",
    "    print(\"Number of classes: {}\".format(NUM_CLASSES))\n",
    "    print(\"=== Model ===\")\n",
    "    print(\"Model class: {}\".format(MODEL_CLS.__name__))\n",
    "    print(\"Model size: {}\".format(MODEL_SIZE))\n",
    "    print(\"Box loss weight: {}\".format(BOX_LOSS_WEIGHT))\n",
    "    print(\"Class loss weight: {}\".format(CLS_LOSS_WEIGHT))\n",
    "    print(\"DFL loss weight: {}\".format(DFL_LOSS_WEIGHT))\n",
    "    print(\"=== Training ===\")\n",
    "    print(\"Start epoch: {}\".format(START_EPOCH))\n",
    "    print(\"Number of epochs: {}\".format(EPOCHS))\n",
    "    print(\"Learning rate: {}\".format(LEARNING_RATE))\n",
    "    print(\"Freeze backbone: {}\".format(FREEZE_BACKBONE))\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(START_EPOCH + 1, EPOCHS + START_EPOCH + 1):\n",
    "    print(\n",
    "        \"{} - epoch {:3d}/{:3d}\".format(\n",
    "            datetime.now(), epoch, EPOCHS + START_EPOCH\n",
    "        )\n",
    "    )\n",
    "    start_ts = time.perf_counter()\n",
    "    loss_metric_list = []\n",
    "    loss_detail_list = []\n",
    "    val_loss_metric_list = []\n",
    "\n",
    "    for idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = (\n",
    "            images.to(device),\n",
    "            labels.to(device),\n",
    "        )\n",
    "        loss_metric = train_step(model, optimizer, images, labels)\n",
    "        loss_metric_list.append(loss_metric)\n",
    "        loss_detail_list.append(yolo_loss._prev_loss.cpu().detach().numpy())\n",
    "\n",
    "        if (\n",
    "            math.isnan(loss_metric)\n",
    "            or math.isinf(loss_metric)\n",
    "            or loss_metric < 0\n",
    "            or (yolo_loss._prev_loss < 0).any()\n",
    "        ):\n",
    "            print(\"Loss is {:.4f}, stop training.\".format(loss_metric))\n",
    "            error_breaking = True\n",
    "            break\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            lr = scheduler.get_lr()[0]\n",
    "            print(\n",
    "                \"epoch {:3d}/{:3d}, batch: {:4d}/{:4d}, loss {:10.4f} [{:3f}, {:3f}, {:3f}], lr {:10.4e}\".format(\n",
    "                    epoch,\n",
    "                    EPOCHS + START_EPOCH,\n",
    "                    idx + 1,\n",
    "                    len(train_loader),\n",
    "                    loss_metric,\n",
    "                    yolo_loss._prev_loss[0],\n",
    "                    yolo_loss._prev_loss[1],\n",
    "                    yolo_loss._prev_loss[2],\n",
    "                    lr,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    if error_breaking:\n",
    "        break\n",
    "\n",
    "    # Scheduler step after each epoch\n",
    "    # scheduler.step()\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(epoch, model, optimizer, CHECKPOINT_DIR, CHECKPOINT_NAME)\n",
    "\n",
    "    # Validation\n",
    "    for idx, (images, labels) in enumerate(val_loader):\n",
    "        images, labels = (\n",
    "            images.to(device),\n",
    "            labels.to(device),\n",
    "        )\n",
    "        val_loss_metric = val_step(model, images, labels)\n",
    "        val_loss_metric_list.append(val_loss_metric)\n",
    "\n",
    "    # Print info\n",
    "    avg_train_loss = sum(loss_metric_list) / len(loss_metric_list)\n",
    "    ave_train_loss_detail = np.mean(loss_detail_list, axis=0)\n",
    "    avg_val_loss = sum(val_loss_metric_list) / len(val_loss_metric_list)\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    print(\n",
    "        \"epoch {:3d}/{:3d}, train loss {:10.4f}, {}, val loss {:10.4f}, lr {:10.4e}, time {:.2f}s\".format(\n",
    "            epoch,\n",
    "            EPOCHS + START_EPOCH,\n",
    "            avg_train_loss,\n",
    "            ave_train_loss_detail,\n",
    "            avg_val_loss,\n",
    "            lr,\n",
    "            time.perf_counter() - start_ts,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    conf_threshold = 0.0\n",
    "    conf_ratio = 0.1\n",
    "    iou_thr = 0.5\n",
    "    suffix = f\"{epoch:03d}_ct{conf_threshold:5.4f}_cr{conf_ratio:5.4f}_iou{iou_thr:5.4f}\"\n",
    "    pred_output_path = os.path.join(\n",
    "        OUTPUT_DIR, f\"yolo_predictions_{suffix}.csv\"\n",
    "    )\n",
    "    eval_output_path = os.path.join(\n",
    "        OUTPUT_DIR, f\"yolo_eval_results_{suffix}.csv\"\n",
    "    )\n",
    "    score = predict_and_evaluate(\n",
    "        model,\n",
    "        TEST_DATA_PATH,\n",
    "        TEST_IMAGE_DIR,\n",
    "        IMAGE_SIZE,\n",
    "        BATCH_SIZE,\n",
    "        pred_output_path,\n",
    "        eval_output_path,\n",
    "        pin_memory=True,\n",
    "        conf_threshold=conf_threshold,\n",
    "        conf_ratio=conf_ratio,\n",
    "        iou_thr=iou_thr,\n",
    "        device=device,\n",
    "    )\n",
    "    print(\n",
    "        \"epoch {:3d}/{:3d}, confidence threshold {:5.4f}, confidence ratio {:5.4f}, IoU threshold {:5.4f}, test mAP {:.4f}\".format(\n",
    "            epoch,\n",
    "            EPOCHS + START_EPOCH,\n",
    "            conf_threshold,\n",
    "            conf_ratio,\n",
    "            iou_thr,\n",
    "            score,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from IPython import get_ipython\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils.data import CLASS_NAMES\n",
    "from models.yolov8.layers import YoloV8\n",
    "from models.yolov8.utils import process_outputs\n",
    "\n",
    "\n",
    "def predict_draw(image_path, model: YoloV8):\n",
    "    np_img = cv2.imread(image_path)\n",
    "    np_img = cv2.resize(np_img, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    np_img = cv2.cvtColor(np_img, cv2.COLOR_BGR2RGB)\n",
    "    resized_img = np_img\n",
    "    np_img = np_img.astype(np.float32)\n",
    "    np_img = np_img / 255.0 * 2 - 1\n",
    "    np_img = np.reshape(np_img, (1, IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    np_img = np.transpose(np_img, (0, 3, 1, 2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        y_pred = (\n",
    "            model.inference(torch.tensor(np_img).to(device)).cpu().detach()\n",
    "        )\n",
    "    bbox_list, class_list, conf_list = process_outputs(\n",
    "        y_pred, IMAGE_SIZE, conf_threshold=0.0, conf_ratio=0.01, iou_thr=0.23\n",
    "    )\n",
    "\n",
    "    bboxes, classes, confidences = (\n",
    "        bbox_list[0],\n",
    "        class_list[0],\n",
    "        conf_list[0],\n",
    "    )\n",
    "    for idx, (bbox, class_idx, conf) in enumerate(\n",
    "        zip(bboxes, classes, confidences)\n",
    "    ):\n",
    "        if idx >= 4:\n",
    "            break\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        cv2.rectangle(\n",
    "            resized_img,\n",
    "            (int(xmin), int(ymin)),\n",
    "            (int(xmax), int(ymax)),\n",
    "            (0, 255, 255),\n",
    "            2,\n",
    "        )\n",
    "        txt = f\"{CLASS_NAMES[class_idx]}: {conf:.2f}\"\n",
    "        cv2.putText(\n",
    "            resized_img,\n",
    "            txt,\n",
    "            (int(xmin) + 8, int(ymin) + 18),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7,\n",
    "            (0, 0, 0),\n",
    "            3,\n",
    "            cv2.LINE_8,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            resized_img,\n",
    "            txt,\n",
    "            (int(xmin) + 8, int(ymin) + 18),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7,\n",
    "            (0, 220, 0),\n",
    "            2,\n",
    "            cv2.LINE_8,\n",
    "        )\n",
    "    return resized_img\n",
    "\n",
    "\n",
    "def visualize(data_path, image_dir, model, shuffle=False):\n",
    "    # Retrieve image names\n",
    "    image_names = []\n",
    "    with open(data_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            image_names.append(line.strip().split(\" \")[0])\n",
    "    image_names = (\n",
    "        random.sample(image_names, 10) if shuffle else image_names[:10]\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for image_name in tqdm(image_names):\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "        image = predict_draw(image_path, model)\n",
    "        results.append((image, image_name))\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    for i, (img, title) in enumerate(results):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "ipy = get_ipython()\n",
    "if ipy is not None:\n",
    "    ipy.run_line_magic(\"matplotlib\", \"inline\")\n",
    "\n",
    "    # Load model from checkpoint\n",
    "    load_checkpoint(\n",
    "        model, CHECKPOINT_DIR, CHECKPOINT_NAME, epoch=None, device=device\n",
    "    )\n",
    "    model.to(device)\n",
    "    visualize(TRAIN_DATA_PATH, TRAIN_IMAGE_DIR, model, shuffle=True)\n",
    "    visualize(TEST_DATA_PATH, TEST_IMAGE_DIR, model, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Prediction Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.multiprocessing import Pool, cpu_count\n",
    "\n",
    "from models.yolov8.evaluate import predict_all, evaluate_all\n",
    "\n",
    "TUNING_EPOCH = 86\n",
    "CACHE_DIR = \"./.cache/{}\".format(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and save to cache dir\n",
    "epoch = TUNING_EPOCH\n",
    "prediction_cache_file = os.path.join(\n",
    "    CACHE_DIR, \"predictions_{:03d}.pt\".format(epoch)\n",
    ")\n",
    "if not os.path.exists(prediction_cache_file):\n",
    "    load_checkpoint(\n",
    "        model, CHECKPOINT_DIR, CHECKPOINT_NAME, epoch=epoch, device=device\n",
    "    )\n",
    "    model.to(device)\n",
    "    image_names, image_heights, image_widths, outputs = predict_all(\n",
    "        model,\n",
    "        TEST_DATA_PATH,\n",
    "        TEST_IMAGE_DIR,\n",
    "        IMAGE_SIZE,\n",
    "        BATCH_SIZE,\n",
    "        pin_memory=True,\n",
    "        device=device,\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(prediction_cache_file), exist_ok=True)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"image_names\": image_names,\n",
    "            \"image_heights\": image_heights,\n",
    "            \"image_widths\": image_widths,\n",
    "            \"outputs\": outputs,\n",
    "        },\n",
    "        prediction_cache_file,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions from cache dir\n",
    "epoch = TUNING_EPOCH\n",
    "prediction_cache_file = os.path.join(\n",
    "    CACHE_DIR, \"predictions_{:03d}.pt\".format(epoch)\n",
    ")\n",
    "assert os.path.exists(\n",
    "    prediction_cache_file\n",
    "), \"Cache file {} not found.\".format(prediction_cache_file)\n",
    "predictions = torch.load(prediction_cache_file, weights_only=True)\n",
    "image_names, image_heights, image_widths, outputs = (\n",
    "    predictions[\"image_names\"],\n",
    "    predictions[\"image_heights\"],\n",
    "    predictions[\"image_widths\"],\n",
    "    predictions[\"outputs\"],\n",
    ")\n",
    "image_heights, image_widths, outputs = (\n",
    "    image_heights.cpu(),\n",
    "    image_widths.cpu(),\n",
    "    outputs.cpu(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function defined for parallel evaluation\n",
    "def one_evaluate(\n",
    "    image_names: list[str],\n",
    "    image_heights: torch.Tensor,\n",
    "    image_widths: torch.Tensor,\n",
    "    outputs: torch.Tensor,\n",
    "    epoch: int,\n",
    "    image_size: int,\n",
    "    output_dir: str,\n",
    "    conf_threshold: float = 0.0,\n",
    "    conf_ratio: float = 0.1,\n",
    "    per_class_conf_ratio: list[float] = None,\n",
    "    iou_thr: float = 0.5,\n",
    "):\n",
    "    suffix = f\"{epoch:03d}_ct{conf_threshold:5.4f}_cr{conf_ratio:5.4f}_iou{iou_thr:5.4f}\"\n",
    "    pred_output_path = os.path.join(\n",
    "        output_dir, f\"yolo_predictions_{suffix}.csv\"\n",
    "    )\n",
    "    eval_output_path = os.path.join(\n",
    "        output_dir, f\"yolo_eval_results_{suffix}.csv\"\n",
    "    )\n",
    "    score = evaluate_all(\n",
    "        image_names,\n",
    "        image_heights,\n",
    "        image_widths,\n",
    "        outputs,\n",
    "        image_size,\n",
    "        pred_output_path,\n",
    "        eval_output_path,\n",
    "        conf_threshold=conf_threshold,\n",
    "        conf_ratio=conf_ratio,\n",
    "        per_class_conf_ratio=per_class_conf_ratio,\n",
    "        iou_thr=iou_thr,\n",
    "    )\n",
    "    if per_class_conf_ratio is not None:\n",
    "        print(\n",
    "            \"evaluation file: {}\\nepoch {:3d}, confidence threshold {:5.4f}, per class confidence ratio {}, IoU threshold {:5.4f}, test mAP {:.4f}\".format(\n",
    "                eval_output_path,\n",
    "                epoch,\n",
    "                conf_threshold,\n",
    "                per_class_conf_ratio,\n",
    "                iou_thr,\n",
    "                score,\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"evaluation file: {}\\nepoch {:3d}, confidence threshold {:5.4f}, confidence ratio {:5.4f}, IoU threshold {:5.4f}, test mAP {:.4f}\".format(\n",
    "                eval_output_path,\n",
    "                epoch,\n",
    "                conf_threshold,\n",
    "                conf_ratio,\n",
    "                iou_thr,\n",
    "                score,\n",
    "            )\n",
    "        )\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Single evaluation\n",
    "\n",
    "# conf_ratio = 0.04\n",
    "# iou_thr = 0.225\n",
    "# one_evaluate(\n",
    "#     image_names,\n",
    "#     image_heights,\n",
    "#     image_widths,\n",
    "#     outputs,\n",
    "#     epoch,\n",
    "#     IMAGE_SIZE,\n",
    "#     OUTPUT_DIR,\n",
    "#     conf_threshold=0.0,\n",
    "#     conf_ratio=conf_ratio,\n",
    "#     iou_thr=iou_thr,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune confidence ratio\n",
    "iou_thr = 0.1975\n",
    "conf_ratio_list = np.linspace(0, 0.04, 11).tolist()\n",
    "args = [\n",
    "    (\n",
    "        image_names,\n",
    "        image_heights,\n",
    "        image_widths,\n",
    "        outputs,\n",
    "        epoch,\n",
    "        IMAGE_SIZE,\n",
    "        OUTPUT_DIR,\n",
    "        0.0,\n",
    "        conf_ratio,\n",
    "        None,\n",
    "        iou_thr,\n",
    "    )\n",
    "    for conf_ratio in conf_ratio_list\n",
    "]\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "with Pool(len(args)) as p:\n",
    "    scores = p.starmap(\n",
    "        one_evaluate,\n",
    "        args,\n",
    "    )\n",
    "torch.set_num_threads(cpu_count())\n",
    "\n",
    "best_idx = np.argmin(scores)\n",
    "best_conf_ratio = conf_ratio_list[best_idx]\n",
    "print(\n",
    "    \"Best confidence ratio: {}, test mAP {:.4f}\".format(\n",
    "        best_conf_ratio, scores[best_idx]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune iou_thr\n",
    "conf_ratio = 0.0200\n",
    "iou_thr_list = np.linspace(0.1, 0.25, 21).tolist()\n",
    "args = [\n",
    "    (\n",
    "        image_names,\n",
    "        image_heights,\n",
    "        image_widths,\n",
    "        outputs,\n",
    "        epoch,\n",
    "        IMAGE_SIZE,\n",
    "        OUTPUT_DIR,\n",
    "        0.0,\n",
    "        conf_ratio,\n",
    "        None,\n",
    "        iou_thr,\n",
    "    )\n",
    "    for iou_thr in iou_thr_list\n",
    "]\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "with Pool(len(args)) as p:\n",
    "    scores = p.starmap(\n",
    "        one_evaluate,\n",
    "        args,\n",
    "    )\n",
    "torch.set_num_threads(cpu_count())\n",
    "\n",
    "best_idx = np.argmin(scores)\n",
    "best_iou_thr = iou_thr_list[best_idx]\n",
    "print(\n",
    "    \"Best IoU threshold: {}, test mAP {:.4f}\".format(\n",
    "        best_iou_thr, scores[best_idx]\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
