{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLab Cup 2: CNN for Object Detection\n",
    "\n",
    "Sao-Hsuan Lin\n",
    "\n",
    "113062532"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# device\n",
    "DEVICE = \"cuda:0\"  # \"cuda:i\" or \"cpu\"\n",
    "\n",
    "# common params\n",
    "IMAGE_SIZE = 300\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 20\n",
    "MAX_OBJECTS_PER_IMAGE = 20\n",
    "\n",
    "# dataset params\n",
    "# TRAIN_DATA_PATH = \"./dataset/pascal_voc_training_data.txt\"\n",
    "# TRAIN_IMAGE_DIR = \"./dataset/VOCdevkit_train/VOC2007/JPEGImages/\"\n",
    "TRAIN_DATA_PATH = \"./dataset/augmented_data.txt\"\n",
    "TRAIN_IMAGE_DIR = \"./dataset/AugmentedImage/\"\n",
    "TEST_DATA_PATH = \"./dataset/pascal_voc_testing_data.txt\"\n",
    "TEST_IMAGE_DIR = \"./dataset/VOCdevkit_test/VOC2007/JPEGImages/\"\n",
    "\n",
    "# model params\n",
    "CELL_SIZE = 7\n",
    "BOXES_PER_CELL = 2\n",
    "OBJECT_SCALE = 1\n",
    "NOOBJECT_SCALE = 0.5\n",
    "CLASS_SCALE = 3\n",
    "COORD_SCALE = 5\n",
    "\n",
    "# training params\n",
    "# set epochs to 0 to prevent training\n",
    "START_EPOCH = 0\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 0\n",
    "FREEZE_BACKBONE = True\n",
    "\n",
    "# checkpoint params\n",
    "CHECKPOINT_DIR = \"./ckpts/yolo-efficientnet-b3/\"\n",
    "CHECKPOINT_NAME = \"yolo_checkpoint\"\n",
    "\n",
    "# evaluation params\n",
    "OUTPUT_DIR = \"./output/yolo-efficientnet-b3/\"\n",
    "PRED_OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"yolo_predictions.csv\")\n",
    "EVAL_OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"yolo_eval_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {gpus}\")\n",
    "    device = torch.device(DEVICE)\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from models.yolo.data import (\n",
    "    TrainDatasetGenerator,\n",
    "    AugmentedTrainDatasetGenerator,\n",
    ")\n",
    "\n",
    "\n",
    "def create_data_loader(\n",
    "    data_path,\n",
    "    image_dir,\n",
    "    batch_size,\n",
    "    max_objects_per_image,\n",
    "    image_size,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    device: str = \"\",\n",
    "):\n",
    "    dataset = TrainDatasetGenerator(\n",
    "        data_path,\n",
    "        image_dir,\n",
    "        max_objects_per_image,\n",
    "        image_size,\n",
    "        # apply_rotation=True,\n",
    "    )\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        pin_memory_device=device,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        pin_memory_device=device,\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from models.yolo.layers import ConvLeakyReLU\n",
    "\n",
    "\n",
    "class YOLOEfficientNetB3(nn.Module):\n",
    "    def __init__(self, num_classes=1470):\n",
    "        super(YOLOEfficientNetB3, self).__init__()\n",
    "\n",
    "        # Load EfficientNetB7 from torchvision, pretrained on ImageNet\n",
    "        efficientnet_b3 = models.efficientnet_b3(\n",
    "            weights=models.EfficientNet_B3_Weights.DEFAULT\n",
    "        )\n",
    "\n",
    "        # Remove the classifier part of EfficientNet (the last fully connected layer)\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(efficientnet_b3.children())[:-2]\n",
    "        )  # [batch_size, 1536, 10, 10]\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.layer1 = ConvLeakyReLU(\n",
    "            1536, 1024, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.layer2 = ConvLeakyReLU(\n",
    "            1024, 1024, kernel_size=3, stride=2, padding=1\n",
    "        )\n",
    "        self.layer3 = ConvLeakyReLU(\n",
    "            1024, 1024, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.layer4 = ConvLeakyReLU(\n",
    "            1024, 1024, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(1024 * 5 * 5, 4096)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "        self.fc2 = nn.Linear(4096, num_classes)\n",
    "\n",
    "        # Xavier initialization for fully connected layers\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.constant_(self.fc2.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through EfficientNetB3 backbone\n",
    "        x = self.backbone(x)  # Output shape: [batch_size, 1536, 10, 10]\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def freeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLOEfficientNetB3().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from datetime import datetime\n",
    "from models.yolo.layers import YoloLossV2\n",
    "from utils.training import load_checkpoint, save_checkpoint\n",
    "\n",
    "# Directory for saving checkpoints\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "train_loader, val_loader = create_data_loader(\n",
    "    TRAIN_DATA_PATH,\n",
    "    TRAIN_IMAGE_DIR,\n",
    "    BATCH_SIZE,\n",
    "    MAX_OBJECTS_PER_IMAGE,\n",
    "    IMAGE_SIZE,\n",
    ")\n",
    "yolo_loss = YoloLossV2(\n",
    "    CELL_SIZE,\n",
    "    NUM_CLASSES,\n",
    "    BOXES_PER_CELL,\n",
    "    IMAGE_SIZE,\n",
    "    CLASS_SCALE,\n",
    "    OBJECT_SCALE,\n",
    "    NOOBJECT_SCALE,\n",
    "    COORD_SCALE,\n",
    "    device,\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "\n",
    "# Training step function\n",
    "def train_step(model, optimizer, images, labels, object_nums):\n",
    "    model.train()  # Set model to training mode\n",
    "    optimizer.zero_grad()  # Zero out gradients\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    class_end = CELL_SIZE * CELL_SIZE * NUM_CLASSES\n",
    "    conf_end = class_end + CELL_SIZE * CELL_SIZE * BOXES_PER_CELL\n",
    "    class_probs = outputs[:, :class_end].view(\n",
    "        -1, CELL_SIZE, CELL_SIZE, NUM_CLASSES\n",
    "    )\n",
    "    confs = outputs[:, class_end:conf_end].view(\n",
    "        -1, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL\n",
    "    )\n",
    "    boxes = outputs[:, conf_end:].view(\n",
    "        -1, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL * 4\n",
    "    )\n",
    "    predicts = torch.cat([class_probs, confs, boxes], dim=3)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = yolo_loss(predicts, labels, object_nums)\n",
    "    loss_metric = loss.item()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss_metric\n",
    "\n",
    "\n",
    "def val_step(model, images, labels, object_nums):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        outputs = model(images)\n",
    "        class_end = CELL_SIZE * CELL_SIZE * NUM_CLASSES\n",
    "        conf_end = class_end + CELL_SIZE * CELL_SIZE * BOXES_PER_CELL\n",
    "        class_probs = outputs[:, :class_end].view(\n",
    "            -1, CELL_SIZE, CELL_SIZE, NUM_CLASSES\n",
    "        )\n",
    "        confs = outputs[:, class_end:conf_end].view(\n",
    "            -1, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL\n",
    "        )\n",
    "        boxes = outputs[:, conf_end:].view(\n",
    "            -1, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL * 4\n",
    "        )\n",
    "        predicts = torch.cat([class_probs, confs, boxes], dim=3)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = yolo_loss(predicts, labels, object_nums)\n",
    "        loss_metric = loss.item()\n",
    "\n",
    "    return loss_metric\n",
    "\n",
    "\n",
    "# Load checkpoint if available\n",
    "if START_EPOCH > 0:\n",
    "    load_checkpoint(\n",
    "        model, CHECKPOINT_DIR, CHECKPOINT_NAME, optimizer, START_EPOCH\n",
    "    )\n",
    "\n",
    "# Set lr\n",
    "optimizer.param_groups[0][\"lr\"] = LEARNING_RATE\n",
    "\n",
    "if FREEZE_BACKBONE:\n",
    "    model.freeze_backbone()\n",
    "else:\n",
    "    model.unfreeze_backbone()\n",
    "\n",
    "# Training loop\n",
    "print(f\"{datetime.now()}, start training.\")\n",
    "for epoch in range(START_EPOCH, EPOCHS + START_EPOCH):\n",
    "    loss_metric_list = []\n",
    "    val_loss_metric_list = []\n",
    "\n",
    "    for idx, (images, labels, object_nums) in enumerate(train_loader):\n",
    "        images, labels, object_nums = (\n",
    "            images.to(device),\n",
    "            labels.to(device),\n",
    "            object_nums.to(device),\n",
    "        )\n",
    "        loss_metric = train_step(model, optimizer, images, labels, object_nums)\n",
    "        loss_metric_list.append(loss_metric)\n",
    "\n",
    "        if (\n",
    "            math.isnan(loss_metric)\n",
    "            or math.isinf(loss_metric)\n",
    "            or loss_metric < 0\n",
    "        ):\n",
    "            print(\"Loss is {:.4f}, stop training.\".format(loss_metric))\n",
    "            break\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            lr = scheduler.get_lr()[0]\n",
    "            print(\n",
    "                \"epoch {:3d}/{:3d}, batch: {:4d}/{:4d}, loss {:10.4f}, lr {:10.4e}\".format(\n",
    "                    epoch + 1,\n",
    "                    EPOCHS + START_EPOCH,\n",
    "                    idx + 1,\n",
    "                    len(train_loader),\n",
    "                    loss_metric,\n",
    "                    lr,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    if math.isnan(loss_metric) or math.isinf(loss_metric) or loss_metric < 0:\n",
    "        break\n",
    "\n",
    "    # # Scheduler step after each epoch\n",
    "    # scheduler.step()\n",
    "\n",
    "    for images, labels, object_nums in val_loader:\n",
    "        images, labels, object_nums = (\n",
    "            images.to(device),\n",
    "            labels.to(device),\n",
    "            object_nums.to(device),\n",
    "        )\n",
    "        val_loss_metric = val_step(model, images, labels, object_nums)\n",
    "        val_loss_metric_list.append(val_loss_metric)\n",
    "\n",
    "    # Print info\n",
    "    avg_train_loss = sum(loss_metric_list) / len(loss_metric_list)\n",
    "    avg_val_loss = sum(val_loss_metric_list) / len(val_loss_metric_list)\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    print(\n",
    "        \"epoch {:3d}/{:3d}, train loss {:10.4f}, val loss {:10.4f}, lr {:10.4e}\".format(\n",
    "            epoch + 1, EPOCHS + START_EPOCH, avg_train_loss, avg_val_loss, lr\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(\n",
    "        epoch + 1, model, optimizer, CHECKPOINT_DIR, CHECKPOINT_NAME\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from checkpoint\n",
    "load_checkpoint(model, CHECKPOINT_DIR, CHECKPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from utils.data import CLASS_NAMES\n",
    "from utils.process_output import process_outputs\n",
    "\n",
    "np_img = cv2.imread('./dataset/VOCdevkit_test/VOC2007/JPEGImages/000002.jpg')\n",
    "resized_img = cv2.resize(np_img, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "np_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB)\n",
    "resized_img = np_img\n",
    "np_img = np_img.astype(np.float32)\n",
    "np_img = np_img / 255.0 * 2 - 1\n",
    "np_img = np.reshape(np_img, (1, IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "np_img = np.transpose(np_img, (0, 3, 1, 2))\n",
    "\n",
    "model.eval()\n",
    "y_pred = model(torch.tensor(np_img).to(device))\n",
    "\n",
    "bboxes, classes, confidences = process_outputs(y_pred, CELL_SIZE, NUM_CLASSES, BOXES_PER_CELL, IMAGE_SIZE, conf_threshold=0.04)\n",
    "for bbox, class_idx, conf in zip(bboxes, classes, confidences):\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "    cv2.rectangle(resized_img, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 255), 3)\n",
    "    txt = f\"{CLASS_NAMES[int(class_idx)]}: {conf:.2f}\"\n",
    "    cv2.putText(resized_img, txt, (int(xmin) + 5, int(ymin) + 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 220, 0), 1, cv2.LINE_8)\n",
    "\n",
    "plt.imshow(resized_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from models.yolo.data import TestDatasetGenerator\n",
    "\n",
    "# Test data loader\n",
    "data_loader = DataLoader(\n",
    "    TestDatasetGenerator(TEST_DATA_PATH, TEST_IMAGE_DIR, IMAGE_SIZE),\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "# Output format: image_name {xmin_i ymin_i xmax_i ymax_i class_i confidence_score} (repeat number of objects times)\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "output_file = open(PRED_OUTPUT_PATH, \"w\")\n",
    "for image_names, images, image_heights, image_widths in data_loader:\n",
    "    images, image_heights, image_widths = (\n",
    "        images.to(device),\n",
    "        image_heights.to(device),\n",
    "        image_widths.to(device),\n",
    "    )\n",
    "    model.eval()\n",
    "    outputs = model(images)\n",
    "\n",
    "    for i in range(images.size(0)):\n",
    "        answers = []\n",
    "        bboxes, classes, confidences = process_outputs(\n",
    "            outputs[i : i + 1],\n",
    "            CELL_SIZE,\n",
    "            NUM_CLASSES,\n",
    "            BOXES_PER_CELL,\n",
    "            IMAGE_SIZE,\n",
    "            conf_threshold=0.03,\n",
    "        )\n",
    "        for bbox, class_idx, conf in zip(bboxes, classes, confidences):\n",
    "            xmin, ymin, xmax, ymax = bbox\n",
    "            xmin, ymin, xmax, ymax = (\n",
    "                xmin * (image_widths[i] / IMAGE_SIZE),\n",
    "                ymin * (image_heights[i] / IMAGE_SIZE),\n",
    "                xmax * (image_widths[i] / IMAGE_SIZE),\n",
    "                ymax * (image_heights[i] / IMAGE_SIZE),\n",
    "            )\n",
    "            answers.append(\n",
    "                \"%d %d %d %d %d %f\" % (xmin, ymin, xmax, ymax, class_idx, conf)\n",
    "            )\n",
    "        output_file.write(image_names[i] + \" \" + \" \".join(answers) + \"\\n\")\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"./evaluate\")\n",
    "\n",
    "import evaluate\n",
    "\n",
    "evaluate.evaluate(PRED_OUTPUT_PATH, EVAL_OUTPUT_PATH)\n",
    "\n",
    "from compute_score import compute_score\n",
    "\n",
    "print(compute_score(EVAL_OUTPUT_PATH))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
